{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10941293,"sourceType":"datasetVersion","datasetId":6804472},{"sourceId":10960150,"sourceType":"datasetVersion","datasetId":6818611},{"sourceId":10962304,"sourceType":"datasetVersion","datasetId":6820180},{"sourceId":10962319,"sourceType":"datasetVersion","datasetId":6820193},{"sourceId":10966483,"sourceType":"datasetVersion","datasetId":6823061},{"sourceId":10967151,"sourceType":"datasetVersion","datasetId":6823555},{"sourceId":10969209,"sourceType":"datasetVersion","datasetId":6825106},{"sourceId":10975496,"sourceType":"datasetVersion","datasetId":6829577},{"sourceId":11004654,"sourceType":"datasetVersion","datasetId":6850701},{"sourceId":11004688,"sourceType":"datasetVersion","datasetId":6850728},{"sourceId":11068904,"sourceType":"datasetVersion","datasetId":6897566},{"sourceId":277131,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":237341,"modelId":259025},{"sourceId":279238,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":239200,"modelId":260859},{"sourceId":279692,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":239594,"modelId":261245},{"sourceId":279993,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":239855,"modelId":261511},{"sourceId":280654,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":240459,"modelId":262105},{"sourceId":288962,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":247583,"modelId":269117},{"sourceId":289077,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":247679,"modelId":269210},{"sourceId":289156,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":247747,"modelId":269276},{"sourceId":289393,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":247944,"modelId":269457},{"sourceId":290273,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":248665,"modelId":270191},{"sourceId":290406,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":248785,"modelId":270315},{"sourceId":290593,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":248964,"modelId":270487},{"sourceId":290946,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":249288,"modelId":270803}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:32:33.370051Z","iopub.execute_input":"2025-03-26T08:32:33.370341Z","iopub.status.idle":"2025-03-26T08:32:33.774187Z","shell.execute_reply.started":"2025-03-26T08:32:33.370321Z","shell.execute_reply":"2025-03-26T08:32:33.773494Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/gnn-v9-9-2/pytorch/default/1/gnn_model_finish9.pth\n/kaggle/input/gnn-v4-2/pytorch/default/1/gnn_model_finish3.pth\n/kaggle/input/test-14/sampled_test_all2.parquet\n/kaggle/input/trainall5/sampled_train_all_5.parquet\n/kaggle/input/trainall3/sampled_train_all_2.parquet\n/kaggle/input/gnn-v3-2/pytorch/default/1/gnn_model_finish2.pth\n/kaggle/input/gnn/pytorch/default/1/gnn_model_finish.pth\n/kaggle/input/gnn-v7-2/pytorch/default/1/gnn_model_finish7.pth\n/kaggle/input/gnn-v3/pytorch/default/1/gnn_model_finish_2.pth\n/kaggle/input/trainall6/sampled_train_all_6.parquet\n/kaggle/input/test-110/sampled_test_all.parquet\n/kaggle/input/gnn-v8-2/pytorch/default/1/gnn_model_finish8-2.pth\n/kaggle/input/gnn-/pytorch/default/1/gnn_model_finish_4.pth\n/kaggle/input/trainall4/sampled_train_all_4.parquet\n/kaggle/input/trainall2/sampled_train_all_3.parquet\n/kaggle/input/gnn-v2/pytorch/default/1/gnn_model_finish.pth\n/kaggle/input/trainall7/sampled_train_all_7.parquet\n/kaggle/input/trainall1/sampled_train_all_1.parquet\n/kaggle/input/gnn-v4/pytorch/default/1/gnn_model_finish_3.pth\n/kaggle/input/trainall0/sampled_train_all.parquet\n/kaggle/input/temp-test/train_transformed__morgan(100k100k).parquet\n/kaggle/input/gnn-v6-2/pytorch/default/1/gnn_model_finish6-2.pth\n/kaggle/input/gnn-v5-2/pytorch/default/1/gnn_model_finish5.pth\n/kaggle/input/gnn-v2-2/pytorch/default/1/gnn_model_finish.pth\n/kaggle/input/leash-BELKA/sample_submission.csv\n/kaggle/input/leash-BELKA/train.parquet\n/kaggle/input/leash-BELKA/test.parquet\n/kaggle/input/leash-BELKA/train.csv\n/kaggle/input/leash-BELKA/test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 1. å®‰è£ PyTorch\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n\n# 2. å®‰è£ RDKitï¼ˆç”¨æ–¼è™•ç†åˆ†å­ï¼‰\n!pip install rdkit-pypi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:32:33.775327Z","iopub.execute_input":"2025-03-26T08:32:33.775833Z","iopub.status.idle":"2025-03-26T08:32:44.367608Z","shell.execute_reply.started":"2025-03-26T08:32:33.775804Z","shell.execute_reply":"2025-03-26T08:32:44.366812Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nCollecting rdkit-pypi\n  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (11.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit-pypi) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit-pypi) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit-pypi) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit-pypi) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit-pypi) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rdkit-pypi) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit-pypi) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rdkit-pypi) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rdkit-pypi) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rdkit-pypi) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rdkit-pypi) (2024.2.0)\nDownloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rdkit-pypi\nSuccessfully installed rdkit-pypi-2022.9.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile requirements.txt\neasydict\nfuture\nmatplotlib\nnumpy\nopencv-python\nscikit-image\nscipy\nclick\nrequests\ntqdm\npyspng\nninja\nimageio-ffmpeg==0.4.3\ntimm\npsutil\nscikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:32:44.369128Z","iopub.execute_input":"2025-03-26T08:32:44.369373Z","iopub.status.idle":"2025-03-26T08:32:44.375048Z","shell.execute_reply.started":"2025-03-26T08:32:44.369351Z","shell.execute_reply":"2025-03-26T08:32:44.374403Z"}},"outputs":[{"name":"stdout","text":"Writing requirements.txt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:32:44.376175Z","iopub.execute_input":"2025-03-26T08:32:44.376396Z","iopub.status.idle":"2025-03-26T08:32:54.414937Z","shell.execute_reply.started":"2025-03-26T08:32:44.376378Z","shell.execute_reply":"2025-03-26T08:32:54.414151Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.13)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.0.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.7.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.26.4)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.10.0.84)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.25.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.13.1)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (8.1.7)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\nCollecting pyspng (from -r requirements.txt (line 11))\n  Downloading pyspng-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.11.1.3)\nCollecting imageio-ffmpeg==0.4.3 (from -r requirements.txt (line 13))\n  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.0.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (5.9.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.2.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->-r requirements.txt (line 4)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->-r requirements.txt (line 4)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->-r requirements.txt (line 4)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->-r requirements.txt (line 4)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->-r requirements.txt (line 4)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->-r requirements.txt (line 4)) (2.4.1)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (3.4.2)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (2.36.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (2024.12.12)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (0.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (2025.1.31)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (0.29.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 16)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 16)) (3.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 14)) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 14)) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 14)) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->-r requirements.txt (line 4)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->-r requirements.txt (line 4)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->-r requirements.txt (line 4)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->-r requirements.txt (line 4)) (2024.2.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 14)) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 14)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm->-r requirements.txt (line 14)) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->-r requirements.txt (line 4)) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 14)) (3.0.2)\nDownloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading pyspng-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: imageio-ffmpeg, pyspng\n  Attempting uninstall: imageio-ffmpeg\n    Found existing installation: imageio-ffmpeg 0.5.1\n    Uninstalling imageio-ffmpeg-0.5.1:\n      Successfully uninstalled imageio-ffmpeg-0.5.1\nSuccessfully installed imageio-ffmpeg-0.4.3 pyspng-0.1.2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 1. å‡ç´š PyTorch å’Œ torchvision\n#!pip install torch==2.1.0 torchvision==0.15.2 torchaudio==2.0.2 torchdata==0.6.1\n!pip install torch torchvision torchaudio torchdata --index-url https://download.pytorch.org/whl/cu118\n!pip install pytorch-lightning\n\n\n# 3. æª¢æŸ¥å®‰è£æ˜¯å¦æˆåŠŸ\nimport torch\nimport torchdata\n\n\nprint(\"Torch version:\", torch.__version__)\nprint(\"Torchdata version:\", torchdata.__version__)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:32:54.415958Z","iopub.execute_input":"2025-03-26T08:32:54.416275Z","iopub.status.idle":"2025-03-26T08:33:06.109362Z","shell.execute_reply.started":"2025-03-26T08:32:54.416245Z","shell.execute_reply":"2025-03-26T08:33:06.108567Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting torchdata\n  Downloading https://download.pytorch.org/whl/torchdata-0.10.0-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/torchdata-0.10.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torchdata\nSuccessfully installed torchdata-0.10.0\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.5.0.post0)\nRequirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.67.1)\nRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.12.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.6.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.12.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.12)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics>=0.7.0->pytorch-lightning) (2024.2.0)\nTorch version: 2.5.1+cu121\nTorchdata version: 0.10.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!python --version\n!nvcc --version  # å¦‚æœå·²ç¶“å®‰è£äº†CUDA\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:06.110115Z","iopub.execute_input":"2025-03-26T08:33:06.110432Z","iopub.status.idle":"2025-03-26T08:33:06.386919Z","shell.execute_reply.started":"2025-03-26T08:33:06.110412Z","shell.execute_reply":"2025-03-26T08:33:06.386123Z"}},"outputs":[{"name":"stdout","text":"Python 3.10.12\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install duckdb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:06.388108Z","iopub.execute_input":"2025-03-26T08:33:06.388438Z","iopub.status.idle":"2025-03-26T08:33:09.569985Z","shell.execute_reply.started":"2025-03-26T08:33:06.388402Z","shell.execute_reply":"2025-03-26T08:33:09.569112Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (1.1.3)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install pyarrow  # æˆ–è€…ä½ ä¹Ÿå¯ä»¥é¸æ“‡ fastparquet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:09.573146Z","iopub.execute_input":"2025-03-26T08:33:09.573391Z","iopub.status.idle":"2025-03-26T08:33:12.805816Z","shell.execute_reply.started":"2025-03-26T08:33:09.573369Z","shell.execute_reply":"2025-03-26T08:33:12.804827Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (19.0.1)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport gc  # å¼•å…¥åƒåœ¾å›æ”¶æ¨¡çµ„\n\nfilename = \"/kaggle/input/leash-BELKA/train.parquet\"\ncolumns_to_read = [\"molecule_smiles\", \"protein_name\", \"binds\"]\n\nbatch_size = 60000    # æ¯æ¬¡è®€å– 60,000 ç­†\ntarget_rows = 1200000  # æ¯è¼ªå„²å­˜ 1,200,000 ç­†\ntotal_batches = 15    # ç¸½å…±åŸ·è¡Œ 15 æ¬¡\ntotal_rows = 0        # è¨˜éŒ„ç•¶å‰ç´¯ç©ç­†æ•¸\n\nparquet_file = pq.ParquetFile(filename)\n\n# ç²å–ç¸½è¡Œçµ„æ•¸é‡\nnum_row_groups = parquet_file.num_row_groups\nprint(f\"Total row groups in file: {num_row_groups}\")\n\n# è¨ˆç®—æ¯æ¬¡è®€å–å¤šå°‘è¡Œçµ„\nrow_groups_per_batch = target_rows // batch_size\n\n\n# é–‹å§‹é€²è¡Œæ‰¹æ¬¡è™•ç†\nfor i in range(total_batches):\n    chunks = []\n    current_rows = 0  # æ¯è¼ªçš„è¨ˆæ•¸å™¨\n    batch_start_row_group = i * row_groups_per_batch  # ç›´æ¥ä¾åºå–\n    batch_end_row_group = min(batch_start_row_group + row_groups_per_batch, num_row_groups)\n\n    if batch_end_row_group >= num_row_groups:\n        batch_end_row_group = num_row_groups  # é¿å…è¶…å‡ºè¡Œçµ„ç¯„åœ\n\n    print(f\"âœ… ç¬¬ {i+1} æ¬¡è™•ç†ï¼šå¾è¡Œçµ„ {batch_start_row_group} åˆ°è¡Œçµ„ {batch_end_row_group}\")\n\n    # ä½¿ç”¨ pyarrow çš„ ParquetFile ç›´æ¥è®€å–æŒ‡å®šç¯„åœçš„è¡Œçµ„\n    for row_group_idx in range(batch_start_row_group, batch_end_row_group):\n        try:\n            batch = parquet_file.read_row_groups([row_group_idx], columns=columns_to_read)\n            chunk = batch.to_pandas()\n\n            # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™\n            if not chunk.empty:\n                chunks.append(chunk)\n                current_rows += len(chunk)\n                total_rows += len(chunk)\n\n            # å¦‚æœè®€å–åˆ°æŒ‡å®šç¯„åœçš„è³‡æ–™ï¼Œå°±åœæ­¢\n            if total_rows >= target_rows * (i + 1):\n                break  # å¦‚æœå·²ç¶“è®€åˆ°è©²æ‰¹æ¬¡çš„çµå°¾å°±åœæ­¢\n\n        except Exception as e:\n            print(f\"âš ï¸ è®€å–è¡Œçµ„ {row_group_idx} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n\n    if chunks:  # ç¢ºä¿æœ‰è³‡æ–™æ‰é€²è¡Œåˆä½µ\n        # åˆä½µ DataFrame\n        batch_df = pd.concat(chunks, ignore_index=True)\n\n        # **å°‡æ¯ 3 åˆ—åˆä½µæˆ 1 åˆ—**\n        batch_df[\"row_idx\"] = batch_df.index // 3  # æ¯ 3 åˆ—åˆ†çµ„\n        batch_pivot = batch_df.pivot(index=\"row_idx\", columns=\"protein_name\", values=\"binds\").reset_index()\n\n        # **åˆä½µ molecule_smiles**\n        smiles_df = batch_df.groupby(\"row_idx\")[\"molecule_smiles\"].first().reset_index()\n        final_df = smiles_df.merge(batch_pivot, on=\"row_idx\").drop(columns=[\"row_idx\"])\n\n        # å­˜æˆ parquetï¼Œæ¯æ¬¡éƒ½å­˜ä¸åŒçš„æª”æ¡ˆ\n        output_filename = f\"/kaggle/working/train_part{i+1}.parquet\"\n        final_df.to_parquet(output_filename, index=False)\n\n        print(f\"âœ… ç¬¬ {i+1} æ¬¡å­˜æª”ï¼š{len(final_df)} ç­†ï¼Œå·²ç´¯ç© {total_rows} ç­†\")\n\n        # æ¸…ç†ç„¡ç”¨çš„è®Šæ•¸ï¼Œé‡‹æ”¾è¨˜æ†¶é«”\n        del batch_df, batch_pivot, smiles_df, final_df\n        gc.collect()  # åŸ·è¡Œåƒåœ¾å›æ”¶\n    else:\n        print(f\"âš ï¸ ç¬¬ {i+1} æ¬¡è™•ç†æœªè®€å–åˆ°ä»»ä½•è³‡æ–™ï¼Œè·³éè©²æ‰¹æ¬¡ã€‚\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:12.807859Z","iopub.execute_input":"2025-03-26T08:33:12.808146Z","iopub.status.idle":"2025-03-26T08:33:12.815712Z","shell.execute_reply.started":"2025-03-26T08:33:12.808123Z","shell.execute_reply":"2025-03-26T08:33:12.814917Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\nimport pyarrow.parquet as pq\\nimport pandas as pd\\nimport gc  # å¼•å…¥åƒåœ¾å›æ”¶æ¨¡çµ„\\n\\nfilename = \"/kaggle/input/leash-BELKA/train.parquet\"\\ncolumns_to_read = [\"molecule_smiles\", \"protein_name\", \"binds\"]\\n\\nbatch_size = 60000    # æ¯æ¬¡è®€å– 60,000 ç­†\\ntarget_rows = 1200000  # æ¯è¼ªå„²å­˜ 1,200,000 ç­†\\ntotal_batches = 15    # ç¸½å…±åŸ·è¡Œ 15 æ¬¡\\ntotal_rows = 0        # è¨˜éŒ„ç•¶å‰ç´¯ç©ç­†æ•¸\\n\\nparquet_file = pq.ParquetFile(filename)\\n\\n# ç²å–ç¸½è¡Œçµ„æ•¸é‡\\nnum_row_groups = parquet_file.num_row_groups\\nprint(f\"Total row groups in file: {num_row_groups}\")\\n\\n# è¨ˆç®—æ¯æ¬¡è®€å–å¤šå°‘è¡Œçµ„\\nrow_groups_per_batch = target_rows // batch_size\\n\\n\\n# é–‹å§‹é€²è¡Œæ‰¹æ¬¡è™•ç†\\nfor i in range(total_batches):\\n    chunks = []\\n    current_rows = 0  # æ¯è¼ªçš„è¨ˆæ•¸å™¨\\n    batch_start_row_group = i * row_groups_per_batch  # ç›´æ¥ä¾åºå–\\n    batch_end_row_group = min(batch_start_row_group + row_groups_per_batch, num_row_groups)\\n\\n    if batch_end_row_group >= num_row_groups:\\n        batch_end_row_group = num_row_groups  # é¿å…è¶…å‡ºè¡Œçµ„ç¯„åœ\\n\\n    print(f\"âœ… ç¬¬ {i+1} æ¬¡è™•ç†ï¼šå¾è¡Œçµ„ {batch_start_row_group} åˆ°è¡Œçµ„ {batch_end_row_group}\")\\n\\n    # ä½¿ç”¨ pyarrow çš„ ParquetFile ç›´æ¥è®€å–æŒ‡å®šç¯„åœçš„è¡Œçµ„\\n    for row_group_idx in range(batch_start_row_group, batch_end_row_group):\\n        try:\\n            batch = parquet_file.read_row_groups([row_group_idx], columns=columns_to_read)\\n            chunk = batch.to_pandas()\\n\\n            # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™\\n            if not chunk.empty:\\n                chunks.append(chunk)\\n                current_rows += len(chunk)\\n                total_rows += len(chunk)\\n\\n            # å¦‚æœè®€å–åˆ°æŒ‡å®šç¯„åœçš„è³‡æ–™ï¼Œå°±åœæ­¢\\n            if total_rows >= target_rows * (i + 1):\\n                break  # å¦‚æœå·²ç¶“è®€åˆ°è©²æ‰¹æ¬¡çš„çµå°¾å°±åœæ­¢\\n\\n        except Exception as e:\\n            print(f\"âš ï¸ è®€å–è¡Œçµ„ {row_group_idx} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\\n\\n    if chunks:  # ç¢ºä¿æœ‰è³‡æ–™æ‰é€²è¡Œåˆä½µ\\n        # åˆä½µ DataFrame\\n        batch_df = pd.concat(chunks, ignore_index=True)\\n\\n        # **å°‡æ¯ 3 åˆ—åˆä½µæˆ 1 åˆ—**\\n        batch_df[\"row_idx\"] = batch_df.index // 3  # æ¯ 3 åˆ—åˆ†çµ„\\n        batch_pivot = batch_df.pivot(index=\"row_idx\", columns=\"protein_name\", values=\"binds\").reset_index()\\n\\n        # **åˆä½µ molecule_smiles**\\n        smiles_df = batch_df.groupby(\"row_idx\")[\"molecule_smiles\"].first().reset_index()\\n        final_df = smiles_df.merge(batch_pivot, on=\"row_idx\").drop(columns=[\"row_idx\"])\\n\\n        # å­˜æˆ parquetï¼Œæ¯æ¬¡éƒ½å­˜ä¸åŒçš„æª”æ¡ˆ\\n        output_filename = f\"/kaggle/working/train_part{i+1}.parquet\"\\n        final_df.to_parquet(output_filename, index=False)\\n\\n        print(f\"âœ… ç¬¬ {i+1} æ¬¡å­˜æª”ï¼š{len(final_df)} ç­†ï¼Œå·²ç´¯ç© {total_rows} ç­†\")\\n\\n        # æ¸…ç†ç„¡ç”¨çš„è®Šæ•¸ï¼Œé‡‹æ”¾è¨˜æ†¶é«”\\n        del batch_df, batch_pivot, smiles_df, final_df\\n        gc.collect()  # åŸ·è¡Œåƒåœ¾å›æ”¶\\n    else:\\n        print(f\"âš ï¸ ç¬¬ {i+1} æ¬¡è™•ç†æœªè®€å–åˆ°ä»»ä½•è³‡æ–™ï¼Œè·³éè©²æ‰¹æ¬¡ã€‚\")\\n'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# 15 å€‹ Parquet æª”æ¡ˆ\nparquet_files = [f\"/kaggle/working/train_part{i+1}.parquet\" for i in range(0, 15)]\n\n# åˆå§‹åŒ– DuckDB é€£ç·š\ncon = duckdb.connect()\n\n# å­˜æ”¾æ‰€æœ‰æ‰¹æ¬¡çš„ DataFrame\nall_samples = []\n\n# é€å€‹è™•ç† 15 å€‹æª”æ¡ˆ\nfor i, file in enumerate(parquet_files):\n    print(f\"ğŸ“‚ æ­£åœ¨è™•ç†æª”æ¡ˆ: {file}\")\n\n    df = con.query(f\"\"\"(SELECT * FROM parquet_scan('{file}')\n                            WHERE BRD4 = 0 and HSA = 0 and sEH = 0\n                            ORDER BY random()\n                            LIMIT 3000)\n                            UNION ALL\n                            (SELECT * FROM parquet_scan('{file}')\n                            WHERE BRD4 = 1 or HSA = 1 or sEH = 1\n                            ORDER BY random()\n                            LIMIT 13000)\"\"\").df()\n\n# å„²å­˜è©²æ‰¹æ¬¡çµæœ\n    output_filename = f\"/kaggle/working/sampled_test_part{i+1}.parquet\"\n    df.to_parquet(output_filename, index=False)\n    print(f\"âœ… å·²å„²å­˜æŠ½æ¨£çµæœ: {output_filename}ï¼ˆå…± {len(df)} ç­†ï¼‰\")\n\n    all_samples.append(df)\n\n# åˆä½µæ‰€æœ‰çµæœ\nfinal_test_df = pd.concat(all_samples, ignore_index=True)\n\n# å„²å­˜ç¸½åˆä½µçš„ Parquet\nfinal_test_output = \"/kaggle/working/sampled_test_all.parquet\"\nfinal_test_df.to_parquet(final_test_output, index=False)\nprint(f\"ğŸ¯ å…¨éƒ¨ 15 å€‹æª”æ¡ˆå·²è™•ç†å®Œç•¢ï¼Œæœ€çµ‚åˆä½µæª”æ¡ˆ: {final_test_output}ï¼ˆå…± {len(final_test_df)} ç­†ï¼‰\")\n\n# é—œé–‰ DuckDB\ncon.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:12.816468Z","iopub.execute_input":"2025-03-26T08:33:12.816698Z","iopub.status.idle":"2025-03-26T08:33:12.835419Z","shell.execute_reply.started":"2025-03-26T08:33:12.816680Z","shell.execute_reply":"2025-03-26T08:33:12.834664Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\n\\n'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"'''\n# 15 å€‹ Parquet æª”æ¡ˆ\nparquet_files = [f\"/kaggle/working/train_part{i+1}.parquet\" for i in range(0, 15)]\n\n# åˆå§‹åŒ– DuckDB é€£ç·š\ncon = duckdb.connect()\n\n# å­˜æ”¾æ‰€æœ‰æ‰¹æ¬¡çš„ DataFrame\nall_samples = []\n\n# é€å€‹è™•ç† 15 å€‹æª”æ¡ˆ\nfor i, file in enumerate(parquet_files):\n    print(f\"ğŸ“‚ æ­£åœ¨è™•ç†æª”æ¡ˆ: {file}\")\n\n    df = con.query(f\"\"\"(SELECT * FROM parquet_scan('{file}')\n                            ORDER BY random()\n                            LIMIT 15000)\n                            UNION ALL\n                            (SELECT * FROM parquet_scan('{file}')\n                            WHERE BRD4 = 1 or HSA = 1 or sEH = 1\n                            ORDER BY random()\n                            LIMIT 5000)\"\"\").df()\n\n# å„²å­˜è©²æ‰¹æ¬¡çµæœ\n    output_filename = f\"/kaggle/working/sampled_test_part{i+1}.parquet\"\n    df.to_parquet(output_filename, index=False)\n    print(f\"âœ… å·²å„²å­˜æŠ½æ¨£çµæœ: {output_filename}ï¼ˆå…± {len(df)} ç­†ï¼‰\")\n\n    all_samples.append(df)\n\n# åˆä½µæ‰€æœ‰çµæœ\nfinal_test_df = pd.concat(all_samples, ignore_index=True)\n\n# å„²å­˜ç¸½åˆä½µçš„ Parquet\nfinal_test_output = \"/kaggle/working/sampled_test_all.parquet\"\nfinal_test_df.to_parquet(final_test_output, index=False)\nprint(f\"ğŸ¯ å…¨éƒ¨ 15 å€‹æª”æ¡ˆå·²è™•ç†å®Œç•¢ï¼Œæœ€çµ‚åˆä½µæª”æ¡ˆ: {final_test_output}ï¼ˆå…± {len(final_test_df)} ç­†ï¼‰\")\n\n# é—œé–‰ DuckDB\ncon.close()\n'''\n#y = df[[\"BRD4\", \"HSA\", \"sEH\"]].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:12.836307Z","iopub.execute_input":"2025-03-26T08:33:12.836605Z","iopub.status.idle":"2025-03-26T08:33:12.848380Z","shell.execute_reply.started":"2025-03-26T08:33:12.836584Z","shell.execute_reply":"2025-03-26T08:33:12.847783Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'\\n# 15 å€‹ Parquet æª”æ¡ˆ\\nparquet_files = [f\"/kaggle/working/train_part{i+1}.parquet\" for i in range(0, 15)]\\n\\n# åˆå§‹åŒ– DuckDB é€£ç·š\\ncon = duckdb.connect()\\n\\n# å­˜æ”¾æ‰€æœ‰æ‰¹æ¬¡çš„ DataFrame\\nall_samples = []\\n\\n# é€å€‹è™•ç† 15 å€‹æª”æ¡ˆ\\nfor i, file in enumerate(parquet_files):\\n    print(f\"ğŸ“‚ æ­£åœ¨è™•ç†æª”æ¡ˆ: {file}\")\\n\\n    df = con.query(f\"\"\"(SELECT * FROM parquet_scan(\\'{file}\\')\\n                            ORDER BY random()\\n                            LIMIT 15000)\\n                            UNION ALL\\n                            (SELECT * FROM parquet_scan(\\'{file}\\')\\n                            WHERE BRD4 = 1 or HSA = 1 or sEH = 1\\n                            ORDER BY random()\\n                            LIMIT 5000)\"\"\").df()\\n\\n# å„²å­˜è©²æ‰¹æ¬¡çµæœ\\n    output_filename = f\"/kaggle/working/sampled_test_part{i+1}.parquet\"\\n    df.to_parquet(output_filename, index=False)\\n    print(f\"âœ… å·²å„²å­˜æŠ½æ¨£çµæœ: {output_filename}ï¼ˆå…± {len(df)} ç­†ï¼‰\")\\n\\n    all_samples.append(df)\\n\\n# åˆä½µæ‰€æœ‰çµæœ\\nfinal_test_df = pd.concat(all_samples, ignore_index=True)\\n\\n# å„²å­˜ç¸½åˆä½µçš„ Parquet\\nfinal_test_output = \"/kaggle/working/sampled_test_all.parquet\"\\nfinal_test_df.to_parquet(final_test_output, index=False)\\nprint(f\"ğŸ¯ å…¨éƒ¨ 15 å€‹æª”æ¡ˆå·²è™•ç†å®Œç•¢ï¼Œæœ€çµ‚åˆä½µæª”æ¡ˆ: {final_test_output}ï¼ˆå…± {len(final_test_df)} ç­†ï¼‰\")\\n\\n# é—œé–‰ DuckDB\\ncon.close()\\n'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:12.849140Z","iopub.execute_input":"2025-03-26T08:33:12.849363Z","iopub.status.idle":"2025-03-26T08:33:12.863062Z","shell.execute_reply.started":"2025-03-26T08:33:12.849346Z","shell.execute_reply":"2025-03-26T08:33:12.862262Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\noutput_filename = f\"/kaggle/working/training_df.csv\"\ndf.to_csv(output_filename, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:12.864072Z","iopub.execute_input":"2025-03-26T08:33:12.864380Z","iopub.status.idle":"2025-03-26T08:33:12.876183Z","shell.execute_reply.started":"2025-03-26T08:33:12.864351Z","shell.execute_reply":"2025-03-26T08:33:12.875372Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'\\noutput_filename = f\"/kaggle/working/training_df.csv\"\\ndf.to_csv(output_filename, index=False)\\n'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"!pip install torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.5.1+cu118.html\n\n!pip install torch-geometric\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.5.0+${CUDA}.html","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:33:12.876943Z","iopub.execute_input":"2025-03-26T08:33:12.877143Z","iopub.status.idle":"2025-03-26T08:46:35.418134Z","shell.execute_reply.started":"2025-03-26T08:33:12.877125Z","shell.execute_reply":"2025-03-26T08:46:35.417233Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu118.html\nCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_sparse-0.6.18%2Bpt25cu118-cp310-cp310-linux_x86_64.whl (5.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_cluster-1.6.3%2Bpt25cu118-cp310-cp310-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hCollecting torch-spline-conv\n  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt25cu118-cp310-cp310-linux_x86_64.whl (947 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m947.7/947.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\nRequirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22.4->scipy->torch-sparse) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22.4->scipy->torch-sparse) (2024.2.0)\nInstalling collected packages: torch-spline-conv, torch-sparse, torch-cluster\nSuccessfully installed torch-cluster-1.6.3+pt25cu118 torch-sparse-0.6.18+pt25cu118 torch-spline-conv-1.2.2+pt25cu118\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.12)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.12.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\nLooking in links: https://data.pyg.org/whl/torch-2.5.0+.html\nCollecting torch-scatter\n  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: torch-scatter\n  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3879861 sha256=118b2ed2c180951d1316cfa8fa1a306c9049665a76b3fac4923a01aa771ab8ab\n  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\nSuccessfully built torch-scatter\nInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # True\nprint(torch.version.cuda)  # æ‡‰è©²é¡¯ç¤º 11.8\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:35.419230Z","iopub.execute_input":"2025-03-26T08:46:35.419458Z","iopub.status.idle":"2025-03-26T08:46:35.442175Z","shell.execute_reply.started":"2025-03-26T08:46:35.419437Z","shell.execute_reply":"2025-03-26T08:46:35.441340Z"}},"outputs":[{"name":"stdout","text":"True\n12.1\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch_scatter\nprint(\"torch-scatter å®‰è£æˆåŠŸï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:35.442986Z","iopub.execute_input":"2025-03-26T08:46:35.443175Z","iopub.status.idle":"2025-03-26T08:46:35.469171Z","shell.execute_reply.started":"2025-03-26T08:46:35.443157Z","shell.execute_reply":"2025-03-26T08:46:35.468569Z"}},"outputs":[{"name":"stdout","text":"torch-scatter å®‰è£æˆåŠŸï¼\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import numpy as np\n\nimport rdkit\nfrom rdkit import Chem\n\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\n\nfrom torch_geometric.nn import MessagePassing, global_mean_pool\nfrom torch_scatter import scatter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint('import ok!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:35.469987Z","iopub.execute_input":"2025-03-26T08:46:35.470284Z","iopub.status.idle":"2025-03-26T08:46:39.519000Z","shell.execute_reply.started":"2025-03-26T08:46:35.470254Z","shell.execute_reply":"2025-03-26T08:46:39.518122Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n","output_type":"stream"},{"name":"stdout","text":"import ok!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# helper\n# torch version of np unpackbits\n#https://gist.github.com/vadimkantorov/30ea6d278bc492abf6ad328c6965613a\n\ndef tensor_dim_slice(tensor, dim, dim_slice):\n\treturn tensor[(dim if dim >= 0 else dim + tensor.dim()) * (slice(None),) + (dim_slice,)]\n\n# @torch.jit.script\ndef packshape(shape, dim: int = -1, mask: int = 0b00000001, dtype=torch.uint8, pack=True):\n\tdim = dim if dim >= 0 else dim + len(shape)\n\tbits, nibble = (\n\t\t8 if dtype is torch.uint8 else 16 if dtype is torch.int16 else 32 if dtype is torch.int32 else 64 if dtype is torch.int64 else 0), (\n\t\t1 if mask == 0b00000001 else 2 if mask == 0b00000011 else 4 if mask == 0b00001111 else 8 if mask == 0b11111111 else 0)\n\t# bits = torch.iinfo(dtype).bits # does not JIT compile\n\tassert nibble <= bits and bits % nibble == 0\n\tnibbles = bits // nibble\n\tshape = (shape[:dim] + (int(math.ceil(shape[dim] / nibbles)),) + shape[1 + dim:]) if pack else (\n\t\t\t\tshape[:dim] + (shape[dim] * nibbles,) + shape[1 + dim:])\n\treturn shape, nibbles, nibble\n\n# @torch.jit.script\ndef F_unpackbits(tensor, dim: int = -1, mask: int = 0b00000001, shape=None, out=None, dtype=torch.uint8):\n\tdim = dim if dim >= 0 else dim + tensor.dim()\n\tshape_, nibbles, nibble = packshape(tensor.shape, dim=dim, mask=mask, dtype=tensor.dtype, pack=False)\n\tshape = shape if shape is not None else shape_\n\tout = out if out is not None else torch.empty(shape, device=tensor.device, dtype=dtype)\n\tassert out.shape == shape\n\n\tif shape[dim] % nibbles == 0:\n\t\tshift = torch.arange((nibbles - 1) * nibble, -1, -nibble, dtype=torch.uint8, device=tensor.device)\n\t\tshift = shift.view(nibbles, *((1,) * (tensor.dim() - dim - 1)))\n\t\treturn torch.bitwise_and((tensor.unsqueeze(1 + dim) >> shift).view_as(out), mask, out=out)\n\n\telse:\n\t\tfor i in range(nibbles):\n\t\t\tshift = nibble * i\n\t\t\tsliced_output = tensor_dim_slice(out, dim, slice(i, None, nibbles))\n\t\t\tsliced_input = tensor.narrow(dim, 0, sliced_output.shape[dim])\n\t\t\ttorch.bitwise_and(sliced_input >> shift, mask, out=sliced_output)\n\treturn out\n\nclass dotdict(dict):\n\t__setattr__ = dict.__setitem__\n\t__delattr__ = dict.__delitem__\n\t\n\tdef __getattr__(self, name):\n\t\ttry:\n\t\t\treturn self[name]\n\t\texcept KeyError:\n\t\t\traise AttributeError(name)\n\n            \nprint('helper ok!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.519912Z","iopub.execute_input":"2025-03-26T08:46:39.520362Z","iopub.status.idle":"2025-03-26T08:46:39.531707Z","shell.execute_reply.started":"2025-03-26T08:46:39.520338Z","shell.execute_reply":"2025-03-26T08:46:39.530864Z"}},"outputs":[{"name":"stdout","text":"helper ok!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# mol to graph adopted from\n# from https://github.com/LiZhang30/GPCNDTA/blob/main/utils/DrugGraph.py\n\nPACK_NODE_DIM=9\nPACK_EDGE_DIM=1\nNODE_DIM=PACK_NODE_DIM*8\nEDGE_DIM=PACK_EDGE_DIM*8\n\ndef one_of_k_encoding(x, allowable_set, allow_unk=False):\n\tif x not in allowable_set:\n\t\tif allow_unk:\n\t\t\tx = allowable_set[-1]\n\t\telse:\n\t\t\traise Exception(f'input {x} not in allowable set{allowable_set}!!!')\n\treturn list(map(lambda s: x == s, allowable_set))\n\n\n#Get features of an atom (one-hot encoding:)\n'''\n\t1.atom element: 44+1 dimensions    \n\t2.the atom's hybridization: 5 dimensions\n\t3.degree of atom: 6 dimensions                        \n\t4.total number of H bound to atom: 6 dimensions\n\t5.number of implicit H bound to atom: 6 dimensions    \n\t6.whether the atom is on ring: 1 dimension\n\t7.whether the atom is aromatic: 1 dimension           \n\tTotal: 70 dimensions\n'''\n\nATOM_SYMBOL = [\n\t'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg',\n\t'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl',\n\t'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H',\n\t'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n\t'Pt', 'Hg', 'Pb', 'Dy',\n\t#'Unknown'\n]\n#print('ATOM_SYMBOL', len(ATOM_SYMBOL))44\nHYBRIDIZATION_TYPE = [\n\tChem.rdchem.HybridizationType.S,\n\tChem.rdchem.HybridizationType.SP,\n\tChem.rdchem.HybridizationType.SP2,\n\tChem.rdchem.HybridizationType.SP3,\n\tChem.rdchem.HybridizationType.SP3D\n]\n\ndef get_atom_feature(atom):\n\tfeature = (\n\t\t one_of_k_encoding(atom.GetSymbol(), ATOM_SYMBOL)\n\t   + one_of_k_encoding(atom.GetHybridization(), HYBRIDIZATION_TYPE)\n\t   + one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5])\n\t   + one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5])\n\t   + one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5])\n\t   + [atom.IsInRing()]\n\t   + [atom.GetIsAromatic()]\n\t)\n\t#feature = np.array(feature, dtype=np.uint8)\n\tfeature = np.packbits(feature)\n\treturn feature\n\n\n#Get features of an edge (one-hot encoding)\n'''\n\t1.single/double/triple/aromatic: 4 dimensions       \n\t2.the atom's hybridization: 1 dimensions\n\t3.whether the bond is on ring: 1 dimension          \n\tTotal: 6 dimensions\n'''\n\ndef get_bond_feature(bond):\n\tbond_type = bond.GetBondType()\n\tfeature = [\n\t\tbond_type == Chem.rdchem.BondType.SINGLE,\n\t\tbond_type == Chem.rdchem.BondType.DOUBLE,\n\t\tbond_type == Chem.rdchem.BondType.TRIPLE,\n\t\tbond_type == Chem.rdchem.BondType.AROMATIC,\n\t\tbond.GetIsConjugated(),\n\t\tbond.IsInRing()\n\t]\n\t#feature = np.array(feature, dtype=np.uint8)\n\tfeature = np.packbits(feature)\n\treturn feature\n\n\ndef smile_to_graph(smiles):\n\tmol = Chem.MolFromSmiles(smiles)\n\tN = mol.GetNumAtoms()\n\tnode_feature = []\n\tedge_feature = []\n\tedge = []\n\tfor i in range(mol.GetNumAtoms()):\n\t\tatom_i = mol.GetAtomWithIdx(i)\n\t\tatom_i_features = get_atom_feature(atom_i)\n\t\tnode_feature.append(atom_i_features)\n\n\t\tfor j in range(mol.GetNumAtoms()):\n\t\t\tbond_ij = mol.GetBondBetweenAtoms(i, j)\n\t\t\tif bond_ij is not None:\n\t\t\t\tedge.append([i, j])\n\t\t\t\tbond_features_ij = get_bond_feature(bond_ij)\n\t\t\t\tedge_feature.append(bond_features_ij)\n\tnode_feature=np.stack(node_feature)\n\tedge_feature=np.stack(edge_feature)\n\tedge = np.array(edge,dtype=np.uint8)\n\treturn N,edge,node_feature,edge_feature\n\ndef to_pyg_format(N,edge,node_feature,edge_feature):\n\tgraph = Data(\n\t\tidx=-1,\n\t\tedge_index = torch.from_numpy(edge.T).int(),\n\t\tx          = torch.from_numpy(node_feature).byte(),\n\t\tedge_attr  = torch.from_numpy(edge_feature).byte(),\n\t)\n\treturn graph\n\n#debug one example\ng = to_pyg_format(*smile_to_graph(smiles=\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\"))\nprint(g)\nprint('[Dy] is replaced by C !!')\nprint('smile_to_graph() ok!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.532729Z","iopub.execute_input":"2025-03-26T08:46:39.533030Z","iopub.status.idle":"2025-03-26T08:46:39.571968Z","shell.execute_reply.started":"2025-03-26T08:46:39.532999Z","shell.execute_reply":"2025-03-26T08:46:39.571323Z"}},"outputs":[{"name":"stdout","text":"Data(x=[37, 9], edge_index=[2, 78], edge_attr=[78, 1], idx=-1)\n[Dy] is replaced by C !!\nsmile_to_graph() ok!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#MODEL: simple MPNNModel\n#from https://github.com/chaitjo/geometric-gnn-dojo/blob/main/geometric_gnn_101.ipynb\n\n#DEVICE='cuda'\nDEVICE='cpu'\n\n# i have removed all comments here to jepp it clean. refer to orginal link for code comments\n# of MPNNModel\nclass MPNNLayer(MessagePassing):\n    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n        super().__init__(aggr=aggr)\n    \n        self.emb_dim = emb_dim\n        self.edge_dim = edge_dim\n        self.mlp_msg = nn.Sequential(\n            nn.Linear(2 * emb_dim + edge_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(),\n            nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU()\n        )\n        self.mlp_upd = nn.Sequential(\n            nn.Linear(2 * emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(),\n            nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU()\n        )\n    \n    def forward(self, h, edge_index, edge_attr):\n        out = self.propagate(edge_index, h=h, edge_attr=edge_attr)\n        return out\n    \n    def message(self, h_i, h_j, edge_attr):\n        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n        return self.mlp_msg(msg)\n    \n    def aggregate(self, inputs, index):\n        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n    \n    def update(self, aggr_out, h):\n        upd_out = torch.cat([h, aggr_out], dim=-1)\n        return self.mlp_upd(upd_out)\n    \n    def __repr__(self) -> str:\n        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n    \n\nclass MPNNModel(nn.Module):\n    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n        super().__init__()\n    \n        self.lin_in = nn.Linear(in_dim, emb_dim)\n    \n        # Stack of MPNN layers\n        self.convs = torch.nn.ModuleList()\n        for layer in range(num_layers):\n            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n    \n        self.pool = global_mean_pool\n    \n    def forward(self, data): #PyG.Data - batch of PyG graphs\n    \n        h = self.lin_in(F_unpackbits(data.x,-1).float())  \n    \n        for conv in self.convs:\n            h = h + conv(h, data.edge_index.long(), F_unpackbits(data.edge_attr,-1).float())  # (n, d) -> (n, d)\n    \n        h_graph = self.pool(h, data.batch)  \n        return h_graph\n\n# our prediction model here !!!!\nclass Net(nn.Module):\n    def __init__(self, ):\n        super().__init__()\n    \n        self.output_type = ['infer', 'loss']\n    \n        graph_dim=96\n        self.smile_encoder = MPNNModel(\n             in_dim=NODE_DIM, edge_dim=EDGE_DIM, emb_dim=graph_dim, num_layers=4,\n        )\n        self.bind = nn.Sequential(\n            nn.Linear(graph_dim, 1024),\n            #nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 1024),\n            #nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 512),\n            #nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(512, 3),\n        )\n    \n    def forward(self, batch):\n        graph = batch['graph']\n        x = self.smile_encoder(graph) \n        bind = self.bind(x)\n    \n        # --------------------------\n        output = {}\n        if 'loss' in self.output_type:\n            target = batch['bind']\n            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n            \"\"\"\n            # Debugging loss calculationæ–°å¢\n            print(\"Bind Output (Sigmoid):\", torch.sigmoid(bind).detach().cpu().numpy())\n            print(\"Target:\", target.cpu().numpy())\n            print(\"Loss Value:\", output['bce_loss'].item())\n            \"\"\"\n    \n        if 'infer' in self.output_type:\n            output['bind'] = torch.sigmoid(bind)\n    \n        return output\n    \n#debug: make some dummy data and run\n'''\ndef run_check_net():\n    batch_size = 30\n    node_dim=NODE_DIM\n    edge_dim=EDGE_DIM\n    \n    data = []\n    for b in range(batch_size):\n        N = np.random.randint(5,10)\n        E = np.random.randint(3,N*(N-1))\n        edge_index = np.stack([\n            np.random.choice(N, E, replace=True),\n            np.random.choice(N, E, replace=True),\n        ]).T\n        edge_index = np.sort(edge_index)\n        edge_index = edge_index[edge_index[:, 0].argsort()]\n        edge_index[0] = [0,1] #default\n        edge_index = edge_index[edge_index[:,0]!=edge_index[:,1]]\n        edge_index = np.unique(edge_index, axis=0)\n    \n        E = len(edge_index)\n        edge_index = np.ascontiguousarray(edge_index.T)\n    \n        d = Data(\n            idx        = b,\n            edge_index = torch.from_numpy(edge_index).int(),\n            x          = torch.from_numpy(np.packbits(np.random.choice(2, (N, node_dim)),-1)).byte(),\n            edge_attr  = torch.from_numpy(np.packbits(np.random.choice(2, (E, edge_dim)),-1)).byte(),\n        )\n        data.append(d)\n    \n    \n    loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n    epoch_indices = []  # å„²å­˜ index ä»¥æª¢æŸ¥è·³è®Š\n    for batch in loader:\n        epoch_indices.extend(batch.idx.tolist())\n    \n    \n    # ğŸ“Œ è¨˜éŒ„ç•¶å‰ epoch æ‰€æœ‰ batch index\n    print(f\"Epoch index range: {min(epoch_indices)} â†’ {max(epoch_indices)}\")\n    \n    \n    # loader = DataLoader(data, batch_size=batch_size)\n    graph = next(iter(loader))\n    idx = graph.idx.tolist()  #use to index bind array\n    batch = dotdict( \n        graph = graph.to(DEVICE),\n        bind  = torch.from_numpy(np.random.choice(2, (batch_size, 3))).float().to(DEVICE),\n    )\n    zz=0\n    \n    net = Net().to(DEVICE)\n    #print(net)\n    \n    with torch.no_grad():\n        with torch.amp.autocast('cuda', enabled=True): # dtype=torch.float16):\n            output = net(batch)\n            #print(output['bind'])\n    \n    # ---\n    print('batch')\n    for k, v in batch.items():\n        if k=='idx':\n            print(f'{k:>32} : {len(v)} ')\n        elif k=='graph':\n            print(f'{k:>32} : {graph} ')\n        else:\n            print(f'{k:>32} : {v.shape} ')\n    \n    print('output')\n    for k, v in output.items():\n        if 'loss' not in k:\n            print(f'{k:>32} : {v.shape} ')\n    print('loss')\n    for k, v in output.items():\n        if 'loss' in k:\n            print(f'{k:>32} : {v.item()} ')\n    \n                \nrun_check_net()\n'''\nprint('model ok!')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.572702Z","iopub.execute_input":"2025-03-26T08:46:39.572916Z","iopub.status.idle":"2025-03-26T08:46:39.586033Z","shell.execute_reply.started":"2025-03-26T08:46:39.572887Z","shell.execute_reply":"2025-03-26T08:46:39.585324Z"}},"outputs":[{"name":"stdout","text":"model ok!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"#example of parallel conversion of smiles to graph\n'''\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nimport gc\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\n\ndef to_pyg_list(graph):\n\tL = len(graph)\n\tfor i in tqdm(range(L)):\n\t\tN, edge, node_feature, edge_feature = graph[i]\n\t\tgraph[i] = Data(\n\t\t\tidx=i,\n\t\t\tedge_index=torch.from_numpy(edge.T).int(),\n\t\t\tx=torch.from_numpy(node_feature).byte(),\n\t\t\tedge_attr=torch.from_numpy(edge_feature).byte(),\n\t\t)\n\treturn graph\n\n\ntrain_smiles=[ #replace [Dy] with C\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n]\ntrain_bind =np.array([\n    [0,0,0],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,0,0],\n])\nnum_train= len(train_smiles)\nwith Pool(processes=64) as pool:\n    train_graph = list(tqdm(pool.imap(smile_to_graph, train_smiles), total=num_train))\n\ntrain_graph = to_pyg_list(train_graph)\ntrain_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True)\n\n## example training loop\nscaler = torch.cuda.amp.GradScaler(enabled=True)\nnet = Net()\nnet.to(DEVICE)\n\noptimizer =\\\n\ttorch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\n\nnum_epoch=10\nepoch=0\niteration=0\nwhile epoch<num_epoch: \n\tfor t, graph_batch in enumerate(train_loader): \n\t\tindex = graph_batch.idx.tolist()\n\t\tB = len(index)\n\t\tbatch = dotdict(\n\t\t\tgraph  = graph_batch.to(DEVICE),\n\t\t\tbind   = torch.from_numpy(train_bind[index]).to(DEVICE),\n\t\t)\n\n\t\tnet.train()\n\t\tnet.output_type = ['loss', 'infer']\n\t\twith torch.cuda.amp.autocast(enabled=True):\n\t\t\toutput = net(batch)  #data_parallel(net,batch) #\n\t\t\tbce_loss = output['bce_loss']\n\n\t\toptimizer.zero_grad() \n\t\tscaler.scale(bce_loss).backward() \n\t\tscaler.step(optimizer)\n\t\tscaler.update()\n\t\t \n\t\ttorch.clear_autocast_cache()\n\t\tprint(epoch,iteration,bce_loss.item())\n\t\titeration +=  1\n        \n\tepoch += 1\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.586718Z","iopub.execute_input":"2025-03-26T08:46:39.586931Z","iopub.status.idle":"2025-03-26T08:46:39.601652Z","shell.execute_reply.started":"2025-03-26T08:46:39.586907Z","shell.execute_reply":"2025-03-26T08:46:39.600886Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'\\nfrom multiprocessing import Pool\\nfrom tqdm import tqdm\\nimport gc\\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\\n\\ndef to_pyg_list(graph):\\n\\tL = len(graph)\\n\\tfor i in tqdm(range(L)):\\n\\t\\tN, edge, node_feature, edge_feature = graph[i]\\n\\t\\tgraph[i] = Data(\\n\\t\\t\\tidx=i,\\n\\t\\t\\tedge_index=torch.from_numpy(edge.T).int(),\\n\\t\\t\\tx=torch.from_numpy(node_feature).byte(),\\n\\t\\t\\tedge_attr=torch.from_numpy(edge_feature).byte(),\\n\\t\\t)\\n\\treturn graph\\n\\n\\ntrain_smiles=[ #replace [Dy] with C\\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\\n]\\ntrain_bind =np.array([\\n    [0,0,0],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,0,0],\\n])\\nnum_train= len(train_smiles)\\nwith Pool(processes=64) as pool:\\n    train_graph = list(tqdm(pool.imap(smile_to_graph, train_smiles), total=num_train))\\n\\ntrain_graph = to_pyg_list(train_graph)\\ntrain_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True)\\n\\n## example training loop\\nscaler = torch.cuda.amp.GradScaler(enabled=True)\\nnet = Net()\\nnet.to(DEVICE)\\n\\noptimizer =\\ttorch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\\n\\nnum_epoch=10\\nepoch=0\\niteration=0\\nwhile epoch<num_epoch: \\n\\tfor t, graph_batch in enumerate(train_loader): \\n\\t\\tindex = graph_batch.idx.tolist()\\n\\t\\tB = len(index)\\n\\t\\tbatch = dotdict(\\n\\t\\t\\tgraph  = graph_batch.to(DEVICE),\\n\\t\\t\\tbind   = torch.from_numpy(train_bind[index]).to(DEVICE),\\n\\t\\t)\\n\\n\\t\\tnet.train()\\n\\t\\tnet.output_type = [\\'loss\\', \\'infer\\']\\n\\t\\twith torch.cuda.amp.autocast(enabled=True):\\n\\t\\t\\toutput = net(batch)  #data_parallel(net,batch) #\\n\\t\\t\\tbce_loss = output[\\'bce_loss\\']\\n\\n\\t\\toptimizer.zero_grad() \\n\\t\\tscaler.scale(bce_loss).backward() \\n\\t\\tscaler.step(optimizer)\\n\\t\\tscaler.update()\\n\\t\\t \\n\\t\\ttorch.clear_autocast_cache()\\n\\t\\tprint(epoch,iteration,bce_loss.item())\\n\\t\\titeration +=  1\\n        \\n\\tepoch += 1\\n'"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"'''\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\n\n\n# è½‰æ›ç‚º PyG æ ¼å¼\ndef to_pyg_list(graph):\n\tL = len(graph)\n\tfor i in tqdm(range(L)):\n\t\tN, edge, node_feature, edge_feature = graph[i]\n\t\tgraph[i] = Data(\n\t\t\tidx=i,\n\t\t\tedge_index=torch.from_numpy(edge.T).int(),\n\t\t\tx=torch.from_numpy(node_feature).byte(),\n\t\t\tedge_attr=torch.from_numpy(edge_feature).byte(),\n\t\t)\n\treturn graph\n\n\ntrain_file = \"/kaggle/input/trainall0/sampled_train_all.parquet\"\ndf = pd.read_parquet(train_file)\n\n\n# train_loader\n# å¾ df è®€å– SMILES\ntrain_smiles = df['molecule_smiles'].tolist()\n\n\ntrain_bind = df[[\"BRD4\", \"HSA\", \"sEH\"]].values\n#train_bind = np.array([[1, 0, 0] if b == 1 else [0, 0, 0] for b in train_bind])  # å‡è¨­ binds æ˜¯ 0/1\n\nnum_train = len(train_smiles)\n\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\n    train_graph = list(tqdm(pool.imap(smile_to_graph, train_smiles), total=num_train))\n\n\ntrain_graph = to_pyg_list(train_graph)\n#train_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True)\ntrain_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True, worker_init_fn=np.random.seed(42))\n\n\n\n# val_loader\n\nval_file = \"/kaggle/working/sampled_train_all.parquet\"\nval_df = pd.read_parquet(val_file)\n\n# å¾ df è®€å– SMILES\nval_smiles = val_df['molecule_smiles'].tolist()\n\n\nval_bind = val_df[[\"BRD4\", \"HSA\", \"sEH\"]].values\n\n\nnum_val = len(val_smiles)\n\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\n    val_graph = list(tqdm(pool.imap(smile_to_graph, val_smiles), total=num_val))\n\n\nval_graph = to_pyg_list(val_graph)\n\nnum_val_samples = 20000  # å– 20,000 ç­†\ntotal_val_samples = len(val_graph)\n\n# éš¨æ©Ÿé¸å– 20,000 å€‹ç´¢å¼•ï¼ˆå¦‚æœæ•¸æ“šå°‘æ–¼ 20,000ï¼Œå‰‡å…¨éƒ¨é¸å–ï¼‰\nrandom_indices = np.random.choice(total_val_samples, min(num_val_samples, total_val_samples), replace=False)\n\n# é¸å–å°æ‡‰çš„ Graph å’Œ Binding Data\nval_graph_subset = [val_graph[i] for i in random_indices]\nval_bind_subset = val_bind[random_indices] \nval_bind =val_bind_subset\n\n\n# å»ºç«‹ DataLoader\nval_loader = PyGDataLoader(val_graph_subset, batch_size=3, shuffle=True)\n\n\n\n## example training loop\nscaler = torch.cuda.amp.GradScaler(enabled=True)\n#net = Net()\nnet = torch.load(\"/kaggle/input/gnn/pytorch/default/1/gnn_model_finish.pth\")\nnet.to(DEVICE)\n\noptimizer =\\\n\ttorch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\n\nnum_epoch=5\nepoch=0\niteration=0\n\n\nwhile epoch<num_epoch: \n    for t, graph_batch in enumerate(train_loader): \n        index = graph_batch.idx.tolist()\n        B = len(index)\n        batch = dotdict(\n\t\t\tgraph  = graph_batch.to(DEVICE),\n\t\t\tbind   = torch.from_numpy(train_bind[index]).to(DEVICE),\n\t\t)\n\n        net.train()\n        net.output_type = ['loss', 'infer']\n        with torch.cuda.amp.autocast(enabled=True):\n            output = net(batch)  #data_parallel(net,batch) \n            bce_loss = output['bce_loss']\n\n        optimizer.zero_grad() \n        scaler.scale(bce_loss).backward() \n        scaler.step(optimizer)\n        scaler.update()\n\t\t \n        torch.clear_autocast_cache()\n        print(epoch,iteration,bce_loss.item())\n        iteration +=  1\n\n    # ===== Validation Step =====\n    net.eval()  # åˆ‡æ›ç‚ºè©•ä¼°æ¨¡å¼\n    val_loss = 0.0\n    num_batches = 0\n    num_samples = 0\n    \n    with torch.no_grad():\n        for batch_idx, val_graph_batch in enumerate(val_loader):  # ä½¿ç”¨ enumerate ç¢ºä¿ batch ç´¢å¼•\n            if num_samples >= num_val_samples:\n                break  # é”åˆ° 50,000 ç­†å¾ŒçµæŸé©—è­‰\n    \n            # é€™è£¡ä¸èƒ½ç”¨ val_graph_batch.idx.tolist()ï¼Œæ”¹æˆç›´æ¥å– batch_idx\n            val_batch = dotdict(\n                graph = val_graph_batch.to(DEVICE),\n                bind = torch.from_numpy(val_bind_subset[batch_idx * 3 : (batch_idx + 1) * 3]).to(DEVICE),\n            )\n    \n            net.output_type = ['loss', 'infer']\n            val_output = net(val_batch)\n            val_loss += val_output['bce_loss'].item()\n            num_batches += 1\n            num_samples += len(val_batch.graph)\n    \n    avg_val_loss = val_loss / num_batches\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n\n    epoch += 1\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.602489Z","iopub.execute_input":"2025-03-26T08:46:39.602752Z","iopub.status.idle":"2025-03-26T08:46:39.615927Z","shell.execute_reply.started":"2025-03-26T08:46:39.602721Z","shell.execute_reply":"2025-03-26T08:46:39.615113Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'\\nimport numpy as np\\nimport torch\\nfrom torch_geometric.data import Data\\nfrom multiprocessing import Pool\\nfrom tqdm import tqdm\\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\\n\\n\\n# è½‰æ›ç‚º PyG æ ¼å¼\\ndef to_pyg_list(graph):\\n\\tL = len(graph)\\n\\tfor i in tqdm(range(L)):\\n\\t\\tN, edge, node_feature, edge_feature = graph[i]\\n\\t\\tgraph[i] = Data(\\n\\t\\t\\tidx=i,\\n\\t\\t\\tedge_index=torch.from_numpy(edge.T).int(),\\n\\t\\t\\tx=torch.from_numpy(node_feature).byte(),\\n\\t\\t\\tedge_attr=torch.from_numpy(edge_feature).byte(),\\n\\t\\t)\\n\\treturn graph\\n\\n\\ntrain_file = \"/kaggle/input/trainall0/sampled_train_all.parquet\"\\ndf = pd.read_parquet(train_file)\\n\\n\\n# train_loader\\n# å¾ df è®€å– SMILES\\ntrain_smiles = df[\\'molecule_smiles\\'].tolist()\\n\\n\\ntrain_bind = df[[\"BRD4\", \"HSA\", \"sEH\"]].values\\n#train_bind = np.array([[1, 0, 0] if b == 1 else [0, 0, 0] for b in train_bind])  # å‡è¨­ binds æ˜¯ 0/1\\n\\nnum_train = len(train_smiles)\\n\\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\\n    train_graph = list(tqdm(pool.imap(smile_to_graph, train_smiles), total=num_train))\\n\\n\\ntrain_graph = to_pyg_list(train_graph)\\n#train_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True)\\ntrain_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True, worker_init_fn=np.random.seed(42))\\n\\n\\n\\n# val_loader\\n\\nval_file = \"/kaggle/working/sampled_train_all.parquet\"\\nval_df = pd.read_parquet(val_file)\\n\\n# å¾ df è®€å– SMILES\\nval_smiles = val_df[\\'molecule_smiles\\'].tolist()\\n\\n\\nval_bind = val_df[[\"BRD4\", \"HSA\", \"sEH\"]].values\\n\\n\\nnum_val = len(val_smiles)\\n\\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\\n    val_graph = list(tqdm(pool.imap(smile_to_graph, val_smiles), total=num_val))\\n\\n\\nval_graph = to_pyg_list(val_graph)\\n\\nnum_val_samples = 20000  # å– 20,000 ç­†\\ntotal_val_samples = len(val_graph)\\n\\n# éš¨æ©Ÿé¸å– 20,000 å€‹ç´¢å¼•ï¼ˆå¦‚æœæ•¸æ“šå°‘æ–¼ 20,000ï¼Œå‰‡å…¨éƒ¨é¸å–ï¼‰\\nrandom_indices = np.random.choice(total_val_samples, min(num_val_samples, total_val_samples), replace=False)\\n\\n# é¸å–å°æ‡‰çš„ Graph å’Œ Binding Data\\nval_graph_subset = [val_graph[i] for i in random_indices]\\nval_bind_subset = val_bind[random_indices] \\nval_bind =val_bind_subset\\n\\n\\n# å»ºç«‹ DataLoader\\nval_loader = PyGDataLoader(val_graph_subset, batch_size=3, shuffle=True)\\n\\n\\n\\n## example training loop\\nscaler = torch.cuda.amp.GradScaler(enabled=True)\\n#net = Net()\\nnet = torch.load(\"/kaggle/input/gnn/pytorch/default/1/gnn_model_finish.pth\")\\nnet.to(DEVICE)\\n\\noptimizer =\\ttorch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\\n\\nnum_epoch=5\\nepoch=0\\niteration=0\\n\\n\\nwhile epoch<num_epoch: \\n    for t, graph_batch in enumerate(train_loader): \\n        index = graph_batch.idx.tolist()\\n        B = len(index)\\n        batch = dotdict(\\n\\t\\t\\tgraph  = graph_batch.to(DEVICE),\\n\\t\\t\\tbind   = torch.from_numpy(train_bind[index]).to(DEVICE),\\n\\t\\t)\\n\\n        net.train()\\n        net.output_type = [\\'loss\\', \\'infer\\']\\n        with torch.cuda.amp.autocast(enabled=True):\\n            output = net(batch)  #data_parallel(net,batch) \\n            bce_loss = output[\\'bce_loss\\']\\n\\n        optimizer.zero_grad() \\n        scaler.scale(bce_loss).backward() \\n        scaler.step(optimizer)\\n        scaler.update()\\n\\t\\t \\n        torch.clear_autocast_cache()\\n        print(epoch,iteration,bce_loss.item())\\n        iteration +=  1\\n\\n    # ===== Validation Step =====\\n    net.eval()  # åˆ‡æ›ç‚ºè©•ä¼°æ¨¡å¼\\n    val_loss = 0.0\\n    num_batches = 0\\n    num_samples = 0\\n    \\n    with torch.no_grad():\\n        for batch_idx, val_graph_batch in enumerate(val_loader):  # ä½¿ç”¨ enumerate ç¢ºä¿ batch ç´¢å¼•\\n            if num_samples >= num_val_samples:\\n                break  # é”åˆ° 50,000 ç­†å¾ŒçµæŸé©—è­‰\\n    \\n            # é€™è£¡ä¸èƒ½ç”¨ val_graph_batch.idx.tolist()ï¼Œæ”¹æˆç›´æ¥å– batch_idx\\n            val_batch = dotdict(\\n                graph = val_graph_batch.to(DEVICE),\\n                bind = torch.from_numpy(val_bind_subset[batch_idx * 3 : (batch_idx + 1) * 3]).to(DEVICE),\\n            )\\n    \\n            net.output_type = [\\'loss\\', \\'infer\\']\\n            val_output = net(val_batch)\\n            val_loss += val_output[\\'bce_loss\\'].item()\\n            num_batches += 1\\n            num_samples += len(val_batch.graph)\\n    \\n    avg_val_loss = val_loss / num_batches\\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\\n\\n    epoch += 1\\n\\n'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# è½‰æ›ç‚º PyG æ ¼å¼\ndef to_pyg_list(graph):\n\tL = len(graph)\n\tfor i in tqdm(range(L)):\n\t\tN, edge, node_feature, edge_feature = graph[i]\n\t\tgraph[i] = Data(\n\t\t\tidx=i,\n\t\t\tedge_index=torch.from_numpy(edge.T).int(),\n\t\t\tx=torch.from_numpy(node_feature).byte(),\n\t\t\tedge_attr=torch.from_numpy(edge_feature).byte(),\n\t\t)\n\treturn graph\n\n\ntrain_file = '/kaggle/input/trainall7/sampled_train_all_7.parquet'\ndf = pd.read_parquet(train_file)\n\n\n# train_loader\n# å¾ df è®€å– SMILES\ntrain_smiles = df['molecule_smiles'].tolist()\n\n\ntrain_bind = df[[\"BRD4\", \"HSA\", \"sEH\"]].values\n#train_bind = np.array([[1, 0, 0] if b == 1 else [0, 0, 0] for b in train_bind])  # å‡è¨­ binds æ˜¯ 0/1\n\nnum_train = len(train_smiles)\n\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\n    train_graph = list(tqdm(pool.imap(smile_to_graph, train_smiles), total=num_train))\n\n\ntrain_graph = to_pyg_list(train_graph)\n#train_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True)\n#train_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True, worker_init_fn=np.random.seed(42))\ntrain_loader = PyGDataLoader(train_graph, batch_size=9, shuffle=True, drop_last=False, worker_init_fn=np.random.seed(42))\n\n\n# val_loader\n\nval_file = \"/kaggle/input/trainall1/sampled_train_all_1.parquet\"\nval_df = pd.read_parquet(val_file)\n\n# å¾ df è®€å– SMILES\nval_smiles = val_df['molecule_smiles'].tolist()\n\n\nval_bind = val_df[[\"BRD4\", \"HSA\", \"sEH\"]].values\n\n\nnum_val = len(val_smiles)\n\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\n    val_graph = list(tqdm(pool.imap(smile_to_graph, val_smiles), total=num_val))\n\n\nval_graph = to_pyg_list(val_graph)\n\nnum_val_samples = 20000  # å– 20,000 ç­†\ntotal_val_samples = len(val_graph)\n\n# éš¨æ©Ÿé¸å– 20,000 å€‹ç´¢å¼•ï¼ˆå¦‚æœæ•¸æ“šå°‘æ–¼ 20,000ï¼Œå‰‡å…¨éƒ¨é¸å–ï¼‰\nrandom_indices = np.random.choice(total_val_samples, min(num_val_samples, total_val_samples), replace=False)\n\n# é¸å–å°æ‡‰çš„ Graph å’Œ Binding Data\nval_graph_subset = [val_graph[i] for i in random_indices]\nval_bind_subset = val_bind[random_indices] \nval_bind =val_bind_subset\n\n\n# å»ºç«‹ DataLoader\nval_loader = PyGDataLoader(val_graph_subset, batch_size=9, shuffle=False, drop_last=False)\n\n\n\n\n## example training loop\nscaler = torch.cuda.amp.GradScaler(enabled=True)\n# scaler = torch.amp.GradScaler(enabled=True)\n\n#net = Net()\nnet = torch.load(\"/kaggle/input/gnn-v8-2/pytorch/default/1/gnn_model_finish8-2.pth\")\nnet.to(DEVICE)\n#net = torch.load(\"/kaggle/input/gnn-/pytorch/default/1/gnn_model_finish_4.pth\", map_location=torch.device('cpu'))\n\n\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=0.0004)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3)\n\nnum_epoch=9\nepoch=0\niteration=0\n\n\nwhile epoch < num_epoch:\n    \n    epoch_loss = 0.0  # ç´€éŒ„æ•´å€‹ epoch çš„ loss\n\n    for t, graph_batch in enumerate(train_loader): \n        epoch_indices = []  # å„²å­˜ index ä»¥æª¢æŸ¥è·³è®Š\n        index = graph_batch.idx.tolist()\n        epoch_indices.extend(index)\n        B = len(index)\n\n        batch = dotdict(\n            graph = graph_batch.to(DEVICE),\n            bind = torch.from_numpy(train_bind[index]).to(DEVICE),\n        )\n\n        # ğŸ“Œ è¨˜éŒ„ç•¶å‰ epoch æ‰€æœ‰ batch index\n        print(f\"Epoch {epoch+1}, Batch {t+1} - Index range: {min(epoch_indices)} â†’ {max(epoch_indices)}\")\n\n        net.train()\n        net.output_type = ['loss', 'infer']\n        \n        with torch.amp.autocast(device_type='cuda', enabled=True):\n            output = net(batch)\n            bce_loss = output['bce_loss']\n\n        optimizer.zero_grad()\n        scaler.scale(bce_loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        torch.cuda.empty_cache()  # æ¸…é™¤ CUDA ç·©å­˜\n\n        # ğŸ“Œ æ›´æ–° epoch ç¸½ loss\n        epoch_loss += bce_loss.item()\n\n        print(f\"Epoch {epoch+1}, Iteration {iteration}, BCE Loss: {bce_loss.item()}\")\n        iteration += 1\n\n    # è¨ˆç®— epoch å¹³å‡ loss ä¸¦æ›´æ–°å­¸ç¿’ç‡\n    epoch_loss /= len(train_loader)  # è¨ˆç®— loss å¹³å‡å€¼\n    scheduler.step(epoch_loss)  # ä½¿ç”¨ ReduceLROnPlateau æ›´æ–° lr\n\n    print(f\"Epoch {epoch+1} çµæŸï¼Œå¹³å‡ BCE Loss: {epoch_loss}, ç•¶å‰å­¸ç¿’ç‡: {optimizer.param_groups[0]['lr']}\")\n    epoch += 1\n\n    \n    # ===== Validation Step =====\n    net.eval()  # åˆ‡æ›ç‚ºè©•ä¼°æ¨¡å¼\n    val_loss = 0.0\n    num_batches = 0\n    num_samples = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch_idx, val_graph_batch in enumerate(val_loader):  # ä½¿ç”¨ enumerate ç¢ºä¿ batch ç´¢å¼•\n            if num_samples >= num_val_samples:\n                break  # é”åˆ° 50,000 ç­†å¾ŒçµæŸé©—è­‰\n    \n            val_batch = dotdict(\n                graph = val_graph_batch.to(DEVICE),\n                bind = torch.from_numpy(val_bind_subset[batch_idx * 9 : (batch_idx + 1) * 9]).to(DEVICE),\n            )\n\n    \n            net.output_type = ['loss', 'infer']\n            val_output = net(val_batch)\n            val_loss += val_output['bce_loss'].item()\n            num_batches += 1\n            num_samples += len(val_batch.graph)\n    \n            # å–å¾—é æ¸¬å€¼ï¼ˆé€šå¸¸ç‚ºæ©Ÿç‡ï¼‰ï¼Œè½‰ç‚º 0/1\n            probs = val_output['bind'].detach().cpu().numpy()  # è½‰ç‚º NumPy\n            preds = (probs > 0.5).astype(int)  # è¨­å®šé–¾å€¼ 0.5\n            labels = val_batch.bind.cpu().numpy()\n    \n            all_preds.append(preds)\n            all_labels.append(labels)\n    \n    # è¨ˆç®—å¹³å‡ Loss\n    avg_val_loss = val_loss / num_batches\n    \n    # å°‡æ‰€æœ‰ batch çš„é æ¸¬å€¼èˆ‡æ¨™ç±¤åˆä½µ\n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    # è¨ˆç®— Accuracy å’Œ F1 Score\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='macro')  # 'macro' è¨ˆç®—æ‰€æœ‰é¡åˆ¥çš„å¹³å‡ F1\n    \n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Validation F1 Score: {f1:.4f}\")\n\n    epoch += 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.619804Z","iopub.execute_input":"2025-03-26T08:46:39.619993Z","iopub.status.idle":"2025-03-26T08:46:39.635076Z","shell.execute_reply.started":"2025-03-26T08:46:39.619976Z","shell.execute_reply":"2025-03-26T08:46:39.634271Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'\\nimport numpy as np\\nimport torch\\nfrom torch_geometric.data import Data\\nfrom multiprocessing import Pool\\nfrom tqdm import tqdm\\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\\nfrom sklearn.metrics import accuracy_score, f1_score\\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\\n\\n# è½‰æ›ç‚º PyG æ ¼å¼\\ndef to_pyg_list(graph):\\n\\tL = len(graph)\\n\\tfor i in tqdm(range(L)):\\n\\t\\tN, edge, node_feature, edge_feature = graph[i]\\n\\t\\tgraph[i] = Data(\\n\\t\\t\\tidx=i,\\n\\t\\t\\tedge_index=torch.from_numpy(edge.T).int(),\\n\\t\\t\\tx=torch.from_numpy(node_feature).byte(),\\n\\t\\t\\tedge_attr=torch.from_numpy(edge_feature).byte(),\\n\\t\\t)\\n\\treturn graph\\n\\n\\ntrain_file = \\'/kaggle/input/trainall7/sampled_train_all_7.parquet\\'\\ndf = pd.read_parquet(train_file)\\n\\n\\n# train_loader\\n# å¾ df è®€å– SMILES\\ntrain_smiles = df[\\'molecule_smiles\\'].tolist()\\n\\n\\ntrain_bind = df[[\"BRD4\", \"HSA\", \"sEH\"]].values\\n#train_bind = np.array([[1, 0, 0] if b == 1 else [0, 0, 0] for b in train_bind])  # å‡è¨­ binds æ˜¯ 0/1\\n\\nnum_train = len(train_smiles)\\n\\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\\n    train_graph = list(tqdm(pool.imap(smile_to_graph, train_smiles), total=num_train))\\n\\n\\ntrain_graph = to_pyg_list(train_graph)\\n#train_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True)\\n#train_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True, worker_init_fn=np.random.seed(42))\\ntrain_loader = PyGDataLoader(train_graph, batch_size=9, shuffle=True, drop_last=False, worker_init_fn=np.random.seed(42))\\n\\n\\n# val_loader\\n\\nval_file = \"/kaggle/input/trainall1/sampled_train_all_1.parquet\"\\nval_df = pd.read_parquet(val_file)\\n\\n# å¾ df è®€å– SMILES\\nval_smiles = val_df[\\'molecule_smiles\\'].tolist()\\n\\n\\nval_bind = val_df[[\"BRD4\", \"HSA\", \"sEH\"]].values\\n\\n\\nnum_val = len(val_smiles)\\n\\n# å¹³è¡Œè™•ç† SMILES è½‰ Graph\\nwith Pool(processes=8) as pool:  # è¨­ç‚º 8 æ ¸å¿ƒï¼Œé¿å…è¨˜æ†¶é«”çˆ†ç‚¸\\n    val_graph = list(tqdm(pool.imap(smile_to_graph, val_smiles), total=num_val))\\n\\n\\nval_graph = to_pyg_list(val_graph)\\n\\nnum_val_samples = 20000  # å– 20,000 ç­†\\ntotal_val_samples = len(val_graph)\\n\\n# éš¨æ©Ÿé¸å– 20,000 å€‹ç´¢å¼•ï¼ˆå¦‚æœæ•¸æ“šå°‘æ–¼ 20,000ï¼Œå‰‡å…¨éƒ¨é¸å–ï¼‰\\nrandom_indices = np.random.choice(total_val_samples, min(num_val_samples, total_val_samples), replace=False)\\n\\n# é¸å–å°æ‡‰çš„ Graph å’Œ Binding Data\\nval_graph_subset = [val_graph[i] for i in random_indices]\\nval_bind_subset = val_bind[random_indices] \\nval_bind =val_bind_subset\\n\\n\\n# å»ºç«‹ DataLoader\\nval_loader = PyGDataLoader(val_graph_subset, batch_size=9, shuffle=False, drop_last=False)\\n\\n\\n\\n\\n## example training loop\\nscaler = torch.cuda.amp.GradScaler(enabled=True)\\n# scaler = torch.amp.GradScaler(enabled=True)\\n\\n#net = Net()\\nnet = torch.load(\"/kaggle/input/gnn-v8-2/pytorch/default/1/gnn_model_finish8-2.pth\")\\nnet.to(DEVICE)\\n#net = torch.load(\"/kaggle/input/gnn-/pytorch/default/1/gnn_model_finish_4.pth\", map_location=torch.device(\\'cpu\\'))\\n\\n\\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=0.0004)\\nscheduler = ReduceLROnPlateau(optimizer, mode=\\'min\\', factor=0.7, patience=3)\\n\\nnum_epoch=9\\nepoch=0\\niteration=0\\n\\n\\nwhile epoch < num_epoch:\\n    \\n    epoch_loss = 0.0  # ç´€éŒ„æ•´å€‹ epoch çš„ loss\\n\\n    for t, graph_batch in enumerate(train_loader): \\n        epoch_indices = []  # å„²å­˜ index ä»¥æª¢æŸ¥è·³è®Š\\n        index = graph_batch.idx.tolist()\\n        epoch_indices.extend(index)\\n        B = len(index)\\n\\n        batch = dotdict(\\n            graph = graph_batch.to(DEVICE),\\n            bind = torch.from_numpy(train_bind[index]).to(DEVICE),\\n        )\\n\\n        # ğŸ“Œ è¨˜éŒ„ç•¶å‰ epoch æ‰€æœ‰ batch index\\n        print(f\"Epoch {epoch+1}, Batch {t+1} - Index range: {min(epoch_indices)} â†’ {max(epoch_indices)}\")\\n\\n        net.train()\\n        net.output_type = [\\'loss\\', \\'infer\\']\\n        \\n        with torch.amp.autocast(device_type=\\'cuda\\', enabled=True):\\n            output = net(batch)\\n            bce_loss = output[\\'bce_loss\\']\\n\\n        optimizer.zero_grad()\\n        scaler.scale(bce_loss).backward()\\n        scaler.step(optimizer)\\n        scaler.update()\\n\\n        torch.cuda.empty_cache()  # æ¸…é™¤ CUDA ç·©å­˜\\n\\n        # ğŸ“Œ æ›´æ–° epoch ç¸½ loss\\n        epoch_loss += bce_loss.item()\\n\\n        print(f\"Epoch {epoch+1}, Iteration {iteration}, BCE Loss: {bce_loss.item()}\")\\n        iteration += 1\\n\\n    # è¨ˆç®— epoch å¹³å‡ loss ä¸¦æ›´æ–°å­¸ç¿’ç‡\\n    epoch_loss /= len(train_loader)  # è¨ˆç®— loss å¹³å‡å€¼\\n    scheduler.step(epoch_loss)  # ä½¿ç”¨ ReduceLROnPlateau æ›´æ–° lr\\n\\n    print(f\"Epoch {epoch+1} çµæŸï¼Œå¹³å‡ BCE Loss: {epoch_loss}, ç•¶å‰å­¸ç¿’ç‡: {optimizer.param_groups[0][\\'lr\\']}\")\\n    epoch += 1\\n\\n    \\n    # ===== Validation Step =====\\n    net.eval()  # åˆ‡æ›ç‚ºè©•ä¼°æ¨¡å¼\\n    val_loss = 0.0\\n    num_batches = 0\\n    num_samples = 0\\n    all_preds = []\\n    all_labels = []\\n    \\n    with torch.no_grad():\\n        for batch_idx, val_graph_batch in enumerate(val_loader):  # ä½¿ç”¨ enumerate ç¢ºä¿ batch ç´¢å¼•\\n            if num_samples >= num_val_samples:\\n                break  # é”åˆ° 50,000 ç­†å¾ŒçµæŸé©—è­‰\\n    \\n            val_batch = dotdict(\\n                graph = val_graph_batch.to(DEVICE),\\n                bind = torch.from_numpy(val_bind_subset[batch_idx * 9 : (batch_idx + 1) * 9]).to(DEVICE),\\n            )\\n\\n    \\n            net.output_type = [\\'loss\\', \\'infer\\']\\n            val_output = net(val_batch)\\n            val_loss += val_output[\\'bce_loss\\'].item()\\n            num_batches += 1\\n            num_samples += len(val_batch.graph)\\n    \\n            # å–å¾—é æ¸¬å€¼ï¼ˆé€šå¸¸ç‚ºæ©Ÿç‡ï¼‰ï¼Œè½‰ç‚º 0/1\\n            probs = val_output[\\'bind\\'].detach().cpu().numpy()  # è½‰ç‚º NumPy\\n            preds = (probs > 0.5).astype(int)  # è¨­å®šé–¾å€¼ 0.5\\n            labels = val_batch.bind.cpu().numpy()\\n    \\n            all_preds.append(preds)\\n            all_labels.append(labels)\\n    \\n    # è¨ˆç®—å¹³å‡ Loss\\n    avg_val_loss = val_loss / num_batches\\n    \\n    # å°‡æ‰€æœ‰ batch çš„é æ¸¬å€¼èˆ‡æ¨™ç±¤åˆä½µ\\n    all_preds = np.vstack(all_preds)\\n    all_labels = np.vstack(all_labels)\\n    \\n    # è¨ˆç®— Accuracy å’Œ F1 Score\\n    accuracy = accuracy_score(all_labels, all_preds)\\n    f1 = f1_score(all_labels, all_preds, average=\\'macro\\')  # \\'macro\\' è¨ˆç®—æ‰€æœ‰é¡åˆ¥çš„å¹³å‡ F1\\n    \\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\\n    print(f\"Validation F1 Score: {f1:.4f}\")\\n\\n    epoch += 1\\n'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"\nmodel_path = \"/kaggle/working/gnn_model_finish.pth\"\ntorch.save(net, model_path)\nprint(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.636712Z","iopub.execute_input":"2025-03-26T08:46:39.636903Z","iopub.status.idle":"2025-03-26T08:46:39.649365Z","shell.execute_reply.started":"2025-03-26T08:46:39.636887Z","shell.execute_reply":"2025-03-26T08:46:39.648768Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'\\nmodel_path = \"/kaggle/working/gnn_model_finish.pth\"\\ntorch.save(net, model_path)\\nprint(f\"æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}\")\\n'"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom torch.utils.data import DataLoader\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nfrom multiprocessing import Pool\n\n# è¨­å®šè¨­å‚™ (GPU or CPU)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# è¼‰å…¥æ¨¡å‹ä¸¦åˆ‡æ›ç‚ºæ¨ç†æ¨¡å¼\nnet = torch.load(\"/kaggle/input/gnn-v9-9-2/pytorch/default/1/gnn_model_finish9.pth\", map_location=DEVICE)\nnet.to(DEVICE)\nnet.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.751079Z","iopub.status.idle":"2025-03-26T08:46:39.751307Z","shell.execute_reply":"2025-03-26T08:46:39.751212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# è¨­å®šè®€å– Parquet æ–‡ä»¶\nfilename = '/kaggle/input/test-110/sampled_test_all.parquet'\ndf = pd.read_parquet(filename)\n\n# æå–æ¸¬è©¦æ•¸æ“š\ntest_smiles = df['molecule_smiles'].tolist()\ny_true = df[[\"BRD4\", \"HSA\", \"sEH\"]].values  # è½‰ç‚º NumPy (Shape: [N, 3])\n\nnum_test = len(test_smiles)\n\n# è½‰æ› SMILES â†’ Graphï¼ˆä½¿ç”¨ 8 æ ¸å¿ƒåŠ é€Ÿï¼‰\nwith Pool(processes=8) as pool:\n    test_graph = list(tqdm(pool.imap(smile_to_graph, test_smiles), total=num_test))\n\n# è½‰æ›ç‚º PyG æ ¼å¼\ntest_graph = to_pyg_list(test_graph)\n\n# å»ºç«‹ PyG DataLoader\ntest_loader = PyGDataLoader(test_graph, batch_size=3, shuffle=False, drop_last=False, worker_init_fn=np.random.seed(42))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.751894Z","iopub.status.idle":"2025-03-26T08:46:39.752202Z","shell.execute_reply":"2025-03-26T08:46:39.752097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\ny_true_df = pd.DataFrame(y_true)  \ny_true_df = y_true_df.fillna(0.0)\ny_true_df.isnull().sum()\ny_true_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.752955Z","iopub.status.idle":"2025-03-26T08:46:39.753323Z","shell.execute_reply":"2025-03-26T08:46:39.753162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== é€²è¡Œæ¨è«– =====\nall_preds = []\nall_probs = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\"):\n        batch = batch.to(DEVICE)  # ç¢ºä¿æ•¸æ“šåœ¨æ­£ç¢ºè¨­å‚™ä¸Š\n        \n        # æ§‹é€ æ¨¡å‹è¼¸å…¥\n        test_batch = {\"graph\": batch}\n\n        # ç²å–æ¨¡å‹è¼¸å‡º\n        net.output_type = ['infer']  # è¨­ç½®ç‚ºæ¨ç†æ¨¡å¼\n        test_output = net(test_batch)\n\n        # ç²å–é æ¸¬æ©Ÿç‡\n        probs = test_output[\"bind\"].detach().cpu().numpy()  # Shape: [batch_size, 3]\n        \n        # è½‰æ›ç‚º 0/1 é æ¸¬å€¼ (é–¾å€¼ 0.5)\n        preds = (probs > 0.5).astype(int)\n\n        # å„²å­˜çµæœ\n        all_probs.append(probs)\n        all_preds.append(preds)\n\n# åˆä½µæ‰€æœ‰ batch çš„é æ¸¬çµæœ\nall_probs = np.vstack(all_probs)  # é æ¸¬æ©Ÿç‡\nall_preds = np.vstack(all_preds)  # äºŒå…ƒé æ¸¬çµæœ\nall_labels = y_true_df.to_numpy()\n\n# ===== è¨ˆç®—æŒ‡æ¨™ =====\naccuracy = accuracy_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds, average='macro')  # 'macro' è¨ˆç®—æ‰€æœ‰é¡åˆ¥çš„å¹³å‡ F1\nauc = roc_auc_score(all_labels, all_probs, average='macro')  # ç›´æ¥ä½¿ç”¨æ©Ÿç‡è¨ˆç®— AUC\n\n# ===== è¼¸å‡ºçµæœ =====\n\nprint(f\"Test F1 Score: {f1:.4f}\")\nprint(f\"Test AUC Score: {auc:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:46:39.754455Z","iopub.status.idle":"2025-03-26T08:46:39.754816Z","shell.execute_reply":"2025-03-26T08:46:39.754671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"submission","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom torch.utils.data import DataLoader\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nfrom multiprocessing import Pool\n\n# è¨­å®šè¨­å‚™ (GPU or CPU)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# è¼‰å…¥æ¨¡å‹ä¸¦åˆ‡æ›ç‚ºæ¨ç†æ¨¡å¼\nnet = torch.load(\"/kaggle/input/gnn-v9-9-2/pytorch/default/1/gnn_model_finish9.pth\", map_location=DEVICE)\nnet.to(DEVICE)\nnet.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:49:45.179230Z","iopub.execute_input":"2025-03-26T08:49:45.179661Z","iopub.status.idle":"2025-03-26T08:49:46.185460Z","shell.execute_reply.started":"2025-03-26T08:49:45.179618Z","shell.execute_reply":"2025-03-26T08:49:46.184747Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-27-507fd60f1ffb>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  net = torch.load(\"/kaggle/input/gnn-v9-9-2/pytorch/default/1/gnn_model_finish9.pth\", map_location=DEVICE)\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Net(\n  (smile_encoder): MPNNModel(\n    (lin_in): Linear(in_features=72, out_features=96, bias=True)\n    (convs): ModuleList(\n      (0-3): 4 x MPNNLayer(emb_dim=96, aggr=add)\n    )\n  )\n  (bind): Sequential(\n    (0): Linear(in_features=96, out_features=1024, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=1024, out_features=1024, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.1, inplace=False)\n    (6): Linear(in_features=1024, out_features=512, bias=True)\n    (7): ReLU(inplace=True)\n    (8): Dropout(p=0.1, inplace=False)\n    (9): Linear(in_features=512, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import duckdb\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np # linear algebra","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:49:46.186726Z","iopub.execute_input":"2025-03-26T08:49:46.187251Z","iopub.status.idle":"2025-03-26T08:49:46.419060Z","shell.execute_reply.started":"2025-03-26T08:49:46.187228Z","shell.execute_reply":"2025-03-26T08:49:46.418392Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import os\n\n# Process the test.parquet file chunk by chunk\ntest_file = '/kaggle/input/leash-BELKA/test.csv'\noutput_file = 'submission_gnn.csv'  # Specify the path and filename for the output file\n\ntest = pd.read_csv(test_file)\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:49:46.420484Z","iopub.execute_input":"2025-03-26T08:49:46.420809Z","iopub.status.idle":"2025-03-26T08:49:52.152364Z","shell.execute_reply.started":"2025-03-26T08:49:46.420786Z","shell.execute_reply":"2025-03-26T08:49:52.151473Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(1674896, 6)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"protein_mapping = {\n    \"BRD4\": 0,\n    \"HSA\": 1,\n    \"sEH\": 2\n}\n\ntest['protein_name'] = test['protein_name'].map(protein_mapping)\n\ntest","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:49:52.153748Z","iopub.execute_input":"2025-03-26T08:49:52.154025Z","iopub.status.idle":"2025-03-26T08:49:52.268543Z","shell.execute_reply.started":"2025-03-26T08:49:52.154004Z","shell.execute_reply":"2025-03-26T08:49:52.267850Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                id                              buildingblock1_smiles  \\\n0        295246830    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n1        295246831    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n2        295246832    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n3        295246833    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n4        295246834    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n...            ...                                                ...   \n1674891  296921721  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n1674892  296921722  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n1674893  296921723  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n1674894  296921724  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n1674895  296921725  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n\n        buildingblock2_smiles   buildingblock3_smiles  \\\n0              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n1              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n2              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n3              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n4              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n...                       ...                     ...   \n1674891     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n1674892     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n1674893     Nc1noc2ccc(F)cc12               NCc1cccs1   \n1674894     Nc1noc2ccc(F)cc12               NCc1cccs1   \n1674895     Nc1noc2ccc(F)cc12               NCc1cccs1   \n\n                                           molecule_smiles  protein_name  \n0        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...             0  \n1        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...             1  \n2        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...             2  \n3        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...             0  \n4        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...             1  \n...                                                    ...           ...  \n1674891  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...             1  \n1674892  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...             2  \n1674893  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...             0  \n1674894  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...             1  \n1674895  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...             2  \n\n[1674896 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>buildingblock1_smiles</th>\n      <th>buildingblock2_smiles</th>\n      <th>buildingblock3_smiles</th>\n      <th>molecule_smiles</th>\n      <th>protein_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>295246830</td>\n      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>295246831</td>\n      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>295246832</td>\n      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>295246833</td>\n      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>295246834</td>\n      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n      <td>C=Cc1ccc(N)cc1</td>\n      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1674891</th>\n      <td>296921721</td>\n      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n      <td>Nc1noc2ccc(F)cc12</td>\n      <td>COC1CCC(CCN)CC1</td>\n      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1674892</th>\n      <td>296921722</td>\n      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n      <td>Nc1noc2ccc(F)cc12</td>\n      <td>COC1CCC(CCN)CC1</td>\n      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1674893</th>\n      <td>296921723</td>\n      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n      <td>Nc1noc2ccc(F)cc12</td>\n      <td>NCc1cccs1</td>\n      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1674894</th>\n      <td>296921724</td>\n      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n      <td>Nc1noc2ccc(F)cc12</td>\n      <td>NCc1cccs1</td>\n      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1674895</th>\n      <td>296921725</td>\n      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n      <td>Nc1noc2ccc(F)cc12</td>\n      <td>NCc1cccs1</td>\n      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>1674896 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"'''\n# æå–æ¸¬è©¦æ•¸æ“š\ntest_smiles = df['molecule_smiles'].tolist()\n\n\nnum_test = len(test_smiles)\n\n# è½‰æ› SMILES â†’ Graphï¼ˆä½¿ç”¨ 8 æ ¸å¿ƒåŠ é€Ÿï¼‰\nwith Pool(processes=8) as pool:\n    test_graph = list(tqdm(pool.imap(smile_to_graph, test_smiles), total=num_test))\n\n# è½‰æ›ç‚º PyG æ ¼å¼\ntest_graph = to_pyg_list(test_graph)\n\n# å»ºç«‹ PyG DataLoader\ntest_loader = PyGDataLoader(test_graph, batch_size=16, shuffle=False, drop_last=False)\n\nall_preds = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Testing\"):\n\n        batch = batch.to(DEVICE)  # ç¢ºä¿æ•¸æ“šåœ¨æ­£ç¢ºè¨­å‚™ä¸Š\n                \n        # æ§‹é€ æ¨¡å‹è¼¸å…¥\n        test_batch = {\"graph\": batch}\n        \n        # ç²å–æ¨¡å‹è¼¸å‡º\n        net.output_type = ['infer']  # è¨­ç½®ç‚ºæ¨ç†æ¨¡å¼\n        test_output = net(test_batch)\n        \n        # ç²å–é æ¸¬æ©Ÿç‡\n        probs = test_output[\"bind\"].detach().cpu().numpy()  # Shape: [batch_size, 3]\n        probs = pd.DataFrame(probs.values.flatten()\n        \n        # è½‰æ›ç‚º 0/1 é æ¸¬å€¼ (é–¾å€¼ 0.5)\n        preds = (probs > 0.5).astype(int)\n        \n        # å„²å­˜çµæœ\n        all_preds.append(preds)\n\n# Create a DataFrame with 'id' and 'probability' columns\noutput_df = pd.DataFrame({'id': test['id'], 'binds': all_preds})\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv(output_file, index=False, mode='a', header=not os.path.exists(output_file))\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:49:52.269314Z","iopub.execute_input":"2025-03-26T08:49:52.269535Z","iopub.status.idle":"2025-03-26T08:49:52.274653Z","shell.execute_reply.started":"2025-03-26T08:49:52.269501Z","shell.execute_reply":"2025-03-26T08:49:52.273883Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'\\n# æå–æ¸¬è©¦æ•¸æ“š\\ntest_smiles = df[\\'molecule_smiles\\'].tolist()\\n\\n\\nnum_test = len(test_smiles)\\n\\n# è½‰æ› SMILES â†’ Graphï¼ˆä½¿ç”¨ 8 æ ¸å¿ƒåŠ é€Ÿï¼‰\\nwith Pool(processes=8) as pool:\\n    test_graph = list(tqdm(pool.imap(smile_to_graph, test_smiles), total=num_test))\\n\\n# è½‰æ›ç‚º PyG æ ¼å¼\\ntest_graph = to_pyg_list(test_graph)\\n\\n# å»ºç«‹ PyG DataLoader\\ntest_loader = PyGDataLoader(test_graph, batch_size=16, shuffle=False, drop_last=False)\\n\\nall_preds = []\\nwith torch.no_grad():\\n    for batch in tqdm(test_loader, desc=\"Testing\"):\\n\\n        batch = batch.to(DEVICE)  # ç¢ºä¿æ•¸æ“šåœ¨æ­£ç¢ºè¨­å‚™ä¸Š\\n                \\n        # æ§‹é€ æ¨¡å‹è¼¸å…¥\\n        test_batch = {\"graph\": batch}\\n        \\n        # ç²å–æ¨¡å‹è¼¸å‡º\\n        net.output_type = [\\'infer\\']  # è¨­ç½®ç‚ºæ¨ç†æ¨¡å¼\\n        test_output = net(test_batch)\\n        \\n        # ç²å–é æ¸¬æ©Ÿç‡\\n        probs = test_output[\"bind\"].detach().cpu().numpy()  # Shape: [batch_size, 3]\\n        probs = pd.DataFrame(probs.values.flatten()\\n        \\n        # è½‰æ›ç‚º 0/1 é æ¸¬å€¼ (é–¾å€¼ 0.5)\\n        preds = (probs > 0.5).astype(int)\\n        \\n        # å„²å­˜çµæœ\\n        all_preds.append(preds)\\n\\n# Create a DataFrame with \\'id\\' and \\'probability\\' columns\\noutput_df = pd.DataFrame({\\'id\\': test[\\'id\\'], \\'binds\\': all_preds})\\n\\n# Save the output DataFrame to a CSV file\\noutput_df.to_csv(output_file, index=False, mode=\\'a\\', header=not os.path.exists(output_file))\\n'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# è½‰æ›ç‚º PyG æ ¼å¼\ndef to_pyg_list(graph):\n\tL = len(graph)\n\tfor i in tqdm(range(L)):\n\t\tN, edge, node_feature, edge_feature = graph[i]\n\t\tgraph[i] = Data(\n\t\t\tidx=i,\n\t\t\tedge_index=torch.from_numpy(edge.T).int(),\n\t\t\tx=torch.from_numpy(node_feature).byte(),\n\t\t\tedge_attr=torch.from_numpy(edge_feature).byte(),\n\t\t)\n\treturn graph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:49:52.275600Z","iopub.execute_input":"2025-03-26T08:49:52.275896Z","iopub.status.idle":"2025-03-26T08:49:52.293921Z","shell.execute_reply.started":"2025-03-26T08:49:52.275864Z","shell.execute_reply":"2025-03-26T08:49:52.292948Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\n\nBATCH_SIZE = 3  # æ¸›å°‘ batch_size ä¾†é™ä½è¨˜æ†¶é«”è² æ“”\nCHUNK_SIZE = 50000  # æ¯æ¬¡è™•ç† 50000 ç­†\nOUTPUT_FILE = \"output.csv\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# **ç¢ºä¿ CSV ä¸æœƒé‡è¤‡å¯«å…¥ header**\nif os.path.exists(OUTPUT_FILE):\n    os.remove(OUTPUT_FILE)  # å…ˆåˆªé™¤ï¼Œé¿å…é‡è¤‡ append\n\n# **é€æ‰¹è™•ç† SMILES**\nfor chunk_start in range(0, len(test), CHUNK_SIZE):\n    chunk_end = min(chunk_start + CHUNK_SIZE, len(test))\n    test_chunk = test.iloc[chunk_start:chunk_end]  # **å–å‡ºä¸€å°éƒ¨åˆ†**\n    \n    test_smiles = test_chunk['molecule_smiles'].tolist()\n    test_protein_names = test_chunk['protein_name'].tolist()\n    test_ids = test_chunk['id'].tolist()\n    \n    # **é€æ‰¹è½‰æ› SMILES â†’ Graph**\n    with Pool(processes=8) as pool:\n        test_graph = list(tqdm(pool.imap(smile_to_graph, test_smiles), total=len(test_smiles)))\n\n    # **è½‰æ›ç‚º PyG æ ¼å¼**\n    test_graph = to_pyg_list(test_graph)\n\n    # **å»ºç«‹ DataLoader**\n    test_loader = PyGDataLoader(test_graph, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n\n    all_preds = []  # **å­˜æ”¾ç•¶å‰ chunk çš„é æ¸¬çµæœ**\n    batch_start = 0  # **æ‰¹æ¬¡ç´¢å¼•è¿½è¹¤**\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f\"Testing {chunk_start}/{len(test)}\"):\n            batch_size = batch.num_graphs\n            batch = batch.to(DEVICE)\n\n            # **æ§‹é€ æ¨¡å‹è¼¸å…¥**\n            test_batch = {\"graph\": batch}\n            net.output_type = ['infer']\n            test_output = net(test_batch)  # **è¼¸å‡ºå½¢ç‹€ [batch_size, num_proteins]**\n\n            for i in range(batch_size):  \n                protein_name = test_protein_names[batch_start + i]\n\n                if protein_name >= test_output[\"bind\"].size(1):\n                    print(f\"Warning: protein_name {protein_name} out of bounds for batch {i}\")\n                    continue\n                \n                protein_pred = test_output[\"bind\"][i, protein_name]\n                all_preds.append(1 if protein_pred > 0.5 else 0)\n\n            batch_start += batch_size\n\n    # **å¯«å…¥ CSVï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨**\n    output_df = pd.DataFrame({'id': test_ids, 'binds': all_preds})\n    output_df.to_csv(OUTPUT_FILE, index=False, mode='a', header=not os.path.exists(OUTPUT_FILE))\n\n    del test_graph, test_loader, all_preds  # **é‡‹æ”¾è¨˜æ†¶é«”**\n    torch.cuda.empty_cache()  # **æ¸…ç† GPU è¨˜æ†¶é«”**\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:49:52.311946Z","iopub.execute_input":"2025-03-26T08:49:52.312274Z","iopub.status.idle":"2025-03-26T10:17:53.315894Z","shell.execute_reply.started":"2025-03-26T08:49:52.312243Z","shell.execute_reply":"2025-03-26T10:17:53.315118Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:02<00:00, 799.44it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 22957.19it/s]\nTesting 0/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:33<00:00, 178.42it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:00<00:00, 830.30it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26014.29it/s]\nTesting 50000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:33<00:00, 177.99it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:02<00:00, 803.75it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26404.38it/s]\nTesting 100000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 179.24it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:00<00:00, 823.59it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 23759.39it/s]\nTesting 150000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:33<00:00, 178.81it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:59<00:00, 845.37it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26321.75it/s]\nTesting 200000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 179.62it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:58<00:00, 861.26it/s] \n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 23758.06it/s]\nTesting 250000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 179.75it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:02<00:00, 805.61it/s] \n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26227.24it/s]\nTesting 300000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 179.83it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:03<00:00, 792.46it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 23966.14it/s]\nTesting 350000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:33<00:00, 178.35it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:05<00:00, 760.58it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26563.65it/s]\nTesting 400000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.05it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:03<00:00, 784.66it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24003.09it/s]\nTesting 450000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.35it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:05<00:00, 764.04it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24160.50it/s]\nTesting 500000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.18it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:03<00:00, 792.32it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24408.89it/s]\nTesting 550000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.92it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:00<00:00, 832.43it/s] \n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24451.48it/s]\nTesting 600000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.60it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:06<00:00, 752.98it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24034.58it/s]\nTesting 650000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.49it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:02<00:00, 802.20it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26812.00it/s]\nTesting 700000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.96it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:00<00:00, 828.10it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24181.47it/s]\nTesting 750000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.73it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:06<00:00, 755.82it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26819.92it/s]\nTesting 800000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.45it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:00<00:00, 827.04it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24107.16it/s]\nTesting 850000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.43it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:05<00:00, 766.92it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 22716.25it/s]\nTesting 900000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.26it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:04<00:00, 770.18it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 25517.01it/s]\nTesting 950000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.18it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:04<00:00, 780.02it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 23614.82it/s]\nTesting 1000000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:04<00:00, 772.73it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26285.27it/s]\nTesting 1050000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.62it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:03<00:00, 785.08it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24697.10it/s]\nTesting 1100000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.09it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:04<00:00, 777.71it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24755.63it/s]\nTesting 1150000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 182.24it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:04<00:00, 777.08it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24626.36it/s]\nTesting 1200000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 182.03it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:03<00:00, 791.00it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24456.09it/s]\nTesting 1250000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.75it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:05<00:00, 760.30it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 23789.09it/s]\nTesting 1300000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.43it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:05<00:00, 761.09it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 26935.48it/s]\nTesting 1350000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.89it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:07<00:00, 737.89it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24218.84it/s]\nTesting 1400000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 182.59it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:59<00:00, 840.77it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 27126.78it/s]\nTesting 1450000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:32<00:00, 180.87it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:59<00:00, 842.52it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24453.32it/s]\nTesting 1500000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 182.81it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:01<00:00, 809.35it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:02<00:00, 24061.22it/s]\nTesting 1550000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.30it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:58<00:00, 850.42it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:01<00:00, 25322.48it/s]\nTesting 1600000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16667/16667 [01:31<00:00, 181.51it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24896/24896 [00:30<00:00, 826.05it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24896/24896 [00:01<00:00, 21721.06it/s]\nTesting 1650000/1674896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8299/8299 [00:45<00:00, 182.40it/s]\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import pandas as pd\n\ninput_file = '/kaggle/working/output.csv'\noutput_file = '/kaggle/working/submission_gnn.csv'\n# è®€å–å·²å„²å­˜çš„ CSV\noutput_df = pd.read_csv(input_file)\n\nprint(len(output_df))\n\n# ä¿®æ”¹ id æ¬„ä½\noutput_df['id'] = range(295246830, 295246830 + len(output_df))\n\nprint(output_df.shape)\n\n# å°‡ä¿®æ”¹å¾Œçš„ DataFrame å„²å­˜å› CSV\noutput_df.to_csv(output_file, index=False, mode='a', header=not os.path.exists(output_file))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T10:58:33.281845Z","iopub.execute_input":"2025-03-26T10:58:33.282172Z","iopub.status.idle":"2025-03-26T10:58:34.485952Z","shell.execute_reply.started":"2025-03-26T10:58:33.282145Z","shell.execute_reply":"2025-03-26T10:58:34.485063Z"}},"outputs":[{"name":"stdout","text":"1674896\n(1674896, 2)\n","output_type":"stream"}],"execution_count":42}]}